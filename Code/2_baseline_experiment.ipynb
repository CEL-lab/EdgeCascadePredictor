{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the processed dataset\n",
    "dataset_path = \"edge_dataset_with_labels.pt\"\n",
    "processed_dataset = torch.load(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.64362.bright04/ipykernel_3406836/1288772817.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load('edge_dataset_with_labels.pt')  # Replace with your path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Weight: 0.5123171043755379, Class 1 Weight: 20.796978281397546\n",
      "Training graphs: 17199, Testing graphs: 4300\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = torch.load('edge_dataset_with_labels.pt')  # Replace with your path\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "label_counter = Counter()\n",
    "for graph in dataset:\n",
    "    label_counter.update(graph.y.tolist())\n",
    "\n",
    "# Class frequencies\n",
    "num_class_0 = label_counter[0]\n",
    "num_class_1 = label_counter[1]\n",
    "total_samples = num_class_0 + num_class_1\n",
    "\n",
    "# Compute class weights\n",
    "weight_0 = total_samples / (2 * num_class_0)\n",
    "weight_1 = total_samples / (2 * num_class_1)\n",
    "class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float)\n",
    "print(f\"Class 0 Weight: {weight_0}, Class 1 Weight: {weight_1}\")\n",
    "\n",
    "# Split dataset into train/test (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training graphs: {len(train_dataset)}, Testing graphs: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU\n",
    "from torch_geometric.nn import NNConv\n",
    "\n",
    "class EdgeFailureGNN(torch.nn.Module):\n",
    "    def __init__(self, in_node_feats, in_edge_feats, hidden_dim, out_dim=2):\n",
    "        super(EdgeFailureGNN, self).__init__()\n",
    "\n",
    "        # Edge-aware MLP for NNConv to compute weights\n",
    "        self.edge_nn1 = Sequential(\n",
    "            Linear(in_edge_feats, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, in_node_feats * hidden_dim)  # 3 * hidden_dim\n",
    "        )\n",
    "        self.edge_nn2 = Sequential(\n",
    "            Linear(in_edge_feats, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim * hidden_dim)  # hidden_dim * hidden_dim\n",
    "        )\n",
    "\n",
    "        # NNConv layers\n",
    "        self.conv1 = NNConv(in_node_feats, hidden_dim, self.edge_nn1, aggr='mean')\n",
    "        self.conv2 = NNConv(hidden_dim, hidden_dim, self.edge_nn2, aggr='mean')\n",
    "\n",
    "        # Edge classification MLP\n",
    "        self.edge_mlp = Sequential(\n",
    "            Linear(hidden_dim * 2, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, out_dim)  # Output: 2 classes (failure or non-failure)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # NNConv layers for node embeddings\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "\n",
    "        # Edge classification: combine node embeddings at edge ends\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([x[row], x[col]], dim=1)  # Concatenate source & target node embeddings\n",
    "\n",
    "        # Pass through edge MLP\n",
    "        return self.edge_mlp(edge_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2358) must match the size of tensor b (786) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_loader)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Loss calculation: match outputs with edge labels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mEdgeFailureGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m     25\u001b[0m     \u001b[39m# Node embedding through NNConv\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_attr))\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index, edge_attr))\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Edge classification: combine node embeddings at edge ends\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:108\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m (x, x)\n\u001b[1;32m    107\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_attr\u001b[39m=\u001b[39;49medge_attr, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    110\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m x_r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m/var/tmp/pbs.64362.bright04/torch_geometric.nn.conv.nn_conv_NNConv_propagate_0zostc0v.py:183\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    174\u001b[0m             kwargs \u001b[39m=\u001b[39m CollectArgs(\n\u001b[1;32m    175\u001b[0m                 x_j\u001b[39m=\u001b[39mhook_kwargs[\u001b[39m'\u001b[39m\u001b[39mx_j\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m                 edge_attr\u001b[39m=\u001b[39mhook_kwargs[\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 dim_size\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mdim_size,\n\u001b[1;32m    180\u001b[0m             )\n\u001b[1;32m    181\u001b[0m \u001b[39m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\n\u001b[1;32m    184\u001b[0m     x_j\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mx_j,\n\u001b[1;32m    185\u001b[0m     edge_attr\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49medge_attr,\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    188\u001b[0m \u001b[39m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:122\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    120\u001b[0m weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn(edge_attr)\n\u001b[1;32m    121\u001b[0m weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_channels_l, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels)\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(x_j\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), weight)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2358) must match the size of tensor b (786) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_loader)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3, 64]' is invalid for input of size 151168",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_loader)\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Loss calculation: match outputs with edge labels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mEdgeFailureGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m     25\u001b[0m     \u001b[39m# Node embedding through NNConv\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_attr))\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index, edge_attr))\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Edge classification: combine node embeddings at edge ends\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:108\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m (x, x)\n\u001b[1;32m    107\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_attr\u001b[39m=\u001b[39;49medge_attr, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    110\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m x_r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m/var/tmp/pbs.64362.bright04/torch_geometric.nn.conv.nn_conv_NNConv_propagate_0zostc0v.py:183\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    174\u001b[0m             kwargs \u001b[39m=\u001b[39m CollectArgs(\n\u001b[1;32m    175\u001b[0m                 x_j\u001b[39m=\u001b[39mhook_kwargs[\u001b[39m'\u001b[39m\u001b[39mx_j\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m                 edge_attr\u001b[39m=\u001b[39mhook_kwargs[\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 dim_size\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mdim_size,\n\u001b[1;32m    180\u001b[0m             )\n\u001b[1;32m    181\u001b[0m \u001b[39m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\n\u001b[1;32m    184\u001b[0m     x_j\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mx_j,\n\u001b[1;32m    185\u001b[0m     edge_attr\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49medge_attr,\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    188\u001b[0m \u001b[39m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:121\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmessage\u001b[39m(\u001b[39mself\u001b[39m, x_j: Tensor, edge_attr: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    120\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn(edge_attr)\n\u001b[0;32m--> 121\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_channels_l, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_channels)\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmatmul(x_j\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), weight)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3, 64]' is invalid for input of size 151168"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:  # Iterate over each batched graph\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        # Loss calculation: match outputs with edge labels\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Train the model for multiple epochs\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_loader)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Step 1: Recompute graph-level labels\n",
    "# A graph is labeled as 1 (positive class) if it contains any failed edges (label == 1)\n",
    "train_graph_labels = [1 if graph.y.sum() > 0 else 0 for graph in train_dataset] \n",
    "\n",
    "# Step 2: Compute class weights (inverse proportional to class counts)\n",
    "class_counts = torch.bincount(torch.tensor(train_graph_labels))\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "print(f\"Class Counts: {class_counts}\")\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Step 3: Assign weights to graphs in the training dataset\n",
    "train_graph_weights = [class_weights[label] for label in train_graph_labels]\n",
    "\n",
    "# Step 4: Create a weighted random sampler for oversampling minority graphs\n",
    "train_sampler = WeightedRandomSampler(train_graph_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "# Step 5: Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Oversampling minority class in the training dataset completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ModuleList, Dropout\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "\n",
    "class EdgePredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(EdgePredictionGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        # Node feature encoder\n",
    "        self.node_encoder = Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Edge feature encoder\n",
    "        self.edge_encoder = Linear(edge_dim, hidden_dim)\n",
    "        \n",
    "        # GNN layers (GATv2Conv supports edge attributes)\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, edge_dim=hidden_dim, add_self_loops=False))\n",
    "        \n",
    "        # Final edge classification layer\n",
    "        self.edge_predictor = Linear(2 * hidden_dim + hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Encode node and edge features\n",
    "        x = self.node_encoder(x)\n",
    "        x = F.relu(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        \n",
    "        # Pass through GNN layers (with edge features)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Compute edge embeddings\n",
    "        row, col = edge_index  # Source and target node indices\n",
    "        edge_embedding = torch.cat([x[row], x[col], edge_attr], dim=1)  # Include edge_attr in the embedding\n",
    "\n",
    "        # Edge classification\n",
    "        logits = self.edge_predictor(edge_embedding)\n",
    "        return logits.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# ==============================\n",
    "# Step 1: Model Initialization\n",
    "# ==============================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model\n",
    "input_dim = train_dataset[0].x.size(-1)  # Node feature dimension\n",
    "edge_dim = train_dataset[0].edge_attr.size(-1)  # Edge feature dimension\n",
    "hidden_dim = 64  # Hidden layer dimension\n",
    "output_dim = 1  # Binary classification (logit output)\n",
    "\n",
    "model = EdgePredictionGNN(\n",
    "    input_dim=input_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Adjust pos_weight for class imbalance at the **edge level**\n",
    "scaled_pos_weight = 10.0  # You can tune this\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(scaled_pos_weight).to(device))\n",
    "print(f\"Adjusted pos_weight: {scaled_pos_weight}\")\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Training Loop\n",
    "# ==============================\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.view(-1), batch.y.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect predictions for metrics\n",
    "        preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(batch.y.cpu())\n",
    "\n",
    "    # Concatenate predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            val_loss += loss_fn(logits.view(-1), batch.y.float()).item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_labels.append(batch.y.cpu())\n",
    "\n",
    "        # Concatenate validation predictions and labels\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        # Compute validation metrics\n",
    "        val_precision = precision_score(val_labels, val_preds, zero_division=0)\n",
    "        val_recall = recall_score(val_labels, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.cat([graph.y for graph in train_dataset])\n",
    "print(f\"Train Edge Label Distribution: {torch.bincount(train_labels)}\")\n",
    "\n",
    "val_labels = torch.cat([graph.y for graph in val_dataset])\n",
    "print(f\"Validation Edge Label Distribution: {torch.bincount(val_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the first 5 graphs in the train_dataset\n",
    "for i in range(5):\n",
    "    graph = train_dataset[i]  # Access the i-th graph from the subset\n",
    "    print(f\"Graph {i}:\")\n",
    "    print(f\" - Node Features: {graph.x}\")\n",
    "    print(f\" - Edge Features: {graph.edge_attr}\")\n",
    "    print(f\" - Edge Index: {graph.edge_index}\")\n",
    "    print(f\" - Edge Labels: {graph.y}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize node and edge features\n",
    "def normalize_features(dataset):\n",
    "    # Collect all node features and edge features\n",
    "    all_node_features = torch.cat([data.x for data in dataset], dim=0)\n",
    "    all_edge_features = torch.cat([data.edge_attr for data in dataset], dim=0)\n",
    "    \n",
    "    # Fit scalers\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    node_scaler.fit(all_node_features.numpy())\n",
    "    edge_scaler.fit(all_edge_features.numpy())\n",
    "\n",
    "    # Apply normalization to the dataset\n",
    "    for data in dataset:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    print(\"Node and Edge features normalized.\")\n",
    "    return dataset\n",
    "\n",
    "# Normalize train, validation, and test datasets\n",
    "train_dataset = normalize_features(train_dataset)\n",
    "val_dataset = normalize_features(val_dataset)\n",
    "test_dataset = normalize_features(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pos_weight based on edge label distribution\n",
    "num_pos = sum([data.y.sum() for data in train_dataset])\n",
    "num_neg = sum([len(data.y) - data.y.sum() for data in train_dataset])\n",
    "\n",
    "pos_weight = num_neg / (num_pos + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "print(f\"Positive Weight (for loss): {pos_weight:.4f}\")\n",
    "\n",
    "# Define the loss function with the updated pos_weight\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "\n",
    "class EdgePredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(EdgePredictionGNN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.edge_nn = Linear(edge_dim, hidden_dim)\n",
    "\n",
    "        # Define NNConv layers for incorporating edge features\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            self.convs.append(NNConv(in_dim, hidden_dim, nn=self.edge_nn, aggr='mean'))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.output = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Total samples requested in WeightedRandomSampler: {len(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.cat([data.y for data in train_dataset], dim=0)\n",
    "print(f\"Total Edge Labels in Train Dataset: {len(train_labels)}\")\n",
    "\n",
    "# Verify the consistency of weights and indices\n",
    "class_counts = torch.bincount(train_labels)\n",
    "class_probs = 1.0 / (class_counts + 1e-6)\n",
    "train_weights = [class_probs[label] for label in train_labels]\n",
    "\n",
    "print(f\"Number of train weights: {len(train_weights)}\")\n",
    "\n",
    "# Ensure the num_samples is valid\n",
    "num_samples = len(train_weights)\n",
    "train_sampler = WeightedRandomSampler(train_weights, num_samples=num_samples, replacement=True)\n",
    "\n",
    "# Check after sampler\n",
    "print(f\"Sampler num_samples: {num_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label a graph as '1' if it contains at least one positive edge, else '0'\n",
    "graph_labels = torch.tensor([1 if (data.y > 0).sum() > 0 else 0 for data in train_dataset])\n",
    "print(f\"Graph-level Labels: {graph_labels}\")\n",
    "\n",
    "# Class counts and class weights for graph-level balancing\n",
    "class_counts = torch.bincount(graph_labels)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "print(f\"Class Counts: {class_counts}\")\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Assign graph weights based on their labels\n",
    "graph_weights = [class_weights[label] for label in graph_labels]\n",
    "\n",
    "# Define the sampler for graph-level oversampling\n",
    "train_sampler = WeightedRandomSampler(graph_weights, num_samples=len(graph_weights), replacement=True)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Oversampling at graph level completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class EdgePredictionGNN(nn.Module):\n",
    "    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim=1, num_layers=3, dropout=0.5):\n",
    "        \"\"\"\n",
    "        GNN-based model for edge label prediction.\n",
    "        Args:\n",
    "            input_dim (int): Dimension of node features.\n",
    "            edge_dim (int): Dimension of edge features.\n",
    "            hidden_dim (int): Hidden dimension for GNN layers.\n",
    "            output_dim (int): Output dimension (default: 1 for binary classification).\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(EdgePredictionGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # GNN layers for processing node features\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            self.convs.append(GCNConv(in_dim, hidden_dim))\n",
    "\n",
    "        # MLP for edge classification\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features [num_nodes, input_dim].\n",
    "            edge_index (torch.Tensor): Edge connectivity [2, num_edges].\n",
    "            edge_attr (torch.Tensor): Edge features [num_edges, edge_dim].\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Predicted logits for edges [num_edges, output_dim].\n",
    "        \"\"\"\n",
    "        # Step 1: Generate node embeddings using GNN layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # Step 2: Combine source and target node embeddings for each edge\n",
    "        edge_src, edge_tgt = edge_index  # Split edge_index into source and target nodes\n",
    "        edge_embeddings = torch.cat([x[edge_src], x[edge_tgt], edge_attr], dim=-1)\n",
    "\n",
    "        # Step 3: Predict edge labels\n",
    "        logits = self.edge_mlp(edge_embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch_geometric.loader import DataLoader  # Correct import for PyG DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# ==============================\n",
    "# Step 1: Normalize Features\n",
    "# ==============================\n",
    "for graph in train_dataset + val_dataset + test_dataset:\n",
    "    graph.x = (graph.x - graph.x.mean(dim=0)) / graph.x.std(dim=0).clamp(min=1e-6)\n",
    "    graph.edge_attr = (graph.edge_attr - graph.edge_attr.mean(dim=0)) / graph.edge_attr.std(dim=0).clamp(min=1e-6)\n",
    "print(\"Normalized node and edge features.\")\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Create DataLoader with Oversampling\n",
    "# ==============================\n",
    "train_graph_labels = [1 if graph.y.sum() > 0 else 0 for graph in train_dataset]  # Class 1 if graph has failed edges\n",
    "class_counts = torch.bincount(torch.tensor(train_graph_labels))\n",
    "class_probs = 1.0 / (class_counts + 1e-6)  # Inverse proportional to class counts\n",
    "train_graph_weights = [class_probs[label] for label in train_graph_labels]\n",
    "train_sampler = WeightedRandomSampler(train_graph_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Oversampling minority class in the training dataset.\")\n",
    "\n",
    "# ==============================\n",
    "# Step 3: Model Initialization\n",
    "# ==============================\n",
    "input_dim = train_dataset[0].x.size(-1)  # Node feature dimension\n",
    "edge_dim = train_dataset[0].edge_attr.size(-1)  # Edge feature dimension\n",
    "hidden_dim = 128  # Increased hidden layer dimension for more capacity\n",
    "output_dim = 1  # Binary classification (logit output)\n",
    "\n",
    "model = EdgePredictionGNN(\n",
    "    input_dim=input_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=4,  # Increased number of layers\n",
    "    dropout=0.5\n",
    ").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-4)  # Added weight decay\n",
    "\n",
    "scaled_pos_weight = 5.0  # Adjusted for handling class imbalance\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(scaled_pos_weight).to(device))\n",
    "print(f\"Adjusted pos_weight: {scaled_pos_weight}\")\n",
    "\n",
    "# ==============================\n",
    "# Step 4: Training Loop\n",
    "# ==============================\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Debug logits: check their distribution\n",
    "        logits_mean = logits.mean().item()\n",
    "        logits_min = logits.min().item()\n",
    "        logits_max = logits.max().item()\n",
    "        print(f\"Epoch {epoch + 1}, Batch Logits - Mean: {logits_mean:.4f}, Min: {logits_min:.4f}, Max: {logits_max:.4f}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.view(-1), batch.y.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect predictions for metrics\n",
    "        preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(batch.y.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    predicted_class_counts = torch.bincount(all_preds)\n",
    "    print(f\"Epoch {epoch + 1}: Predicted Class Distribution: {predicted_class_counts}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            val_logits_mean = logits.mean().item()\n",
    "            val_logits_min = logits.min().item()\n",
    "            val_logits_max = logits.max().item()\n",
    "            print(f\"Epoch {epoch + 1}, Validation Batch Logits - Mean: {val_logits_mean:.4f}, Min: {val_logits_min:.4f}, Max: {val_logits_max:.4f}\")\n",
    "\n",
    "            val_loss += loss_fn(logits.view(-1), batch.y.float()).item()\n",
    "            preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_labels.append(batch.y.cpu())\n",
    "\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        val_precision = precision_score(val_labels, val_preds, zero_division=0)\n",
    "        val_recall = recall_score(val_labels, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted class distribution\n",
    "predicted_class_counts = torch.bincount(all_preds)\n",
    "print(f\"Predicted Class Distribution: {predicted_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the model\n",
    "input_dim = train_dataset[0].x.size(-1)  # Node feature dimension\n",
    "edge_dim = train_dataset[0].edge_attr.size(-1)  # Edge feature dimension\n",
    "hidden_dim = 64  # Hidden layer dimension\n",
    "output_dim = 1  # Binary classification (logit output)\n",
    "\n",
    "model = EdgePredictionGNN(\n",
    "    input_dim=input_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define optimizer and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#loss_fn = nn.BCEWithLogitsLoss()  # Use weighted loss if needed\n",
    "# Adjust BCEWithLogitsLoss to handle imbalance\n",
    "#loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(class_weights[1]).to(device))\n",
    "# Adjust the pos_weight value to a more reasonable scale\n",
    "# Scale down pos_weight further\n",
    "scaled_pos_weight = 5.0  # Use a lower weight to reduce bias toward Class 1\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(scaled_pos_weight).to(device))\n",
    "print(f\"Adjusted pos_weight: {scaled_pos_weight}\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.view(-1), batch.y.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect predictions for metrics\n",
    "        preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(batch.y.cpu())\n",
    "\n",
    "    # Concatenate predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            val_loss += loss_fn(logits.view(-1), batch.y.float()).item()\n",
    "\n",
    "            # Collect validation predictions and labels\n",
    "            preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_labels.append(batch.y.cpu())\n",
    "\n",
    "        # Concatenate validation predictions and labels\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        # Compute validation metrics\n",
    "        val_precision = precision_score(val_labels, val_preds, zero_division=0)\n",
    "        val_recall = recall_score(val_labels, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class EdgeClassifierWithEdgeFeatures(torch.nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels):\n",
    "        super(EdgeClassifierWithEdgeFeatures, self).__init__()\n",
    "        self.conv1 = SAGEConv(node_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.edge_fc1 = torch.nn.Linear(hidden_channels * 2 + edge_in_channels, hidden_channels)\n",
    "        self.edge_fc2 = torch.nn.Linear(hidden_channels, 1)  # Single output for BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        src, dst = edge_index\n",
    "        edge_node_features = torch.cat([x[src], x[dst]], dim=-1)\n",
    "        edge_features = torch.cat([edge_node_features, edge_attr], dim=-1)\n",
    "\n",
    "        edge_features = F.relu(self.edge_fc1(edge_features))\n",
    "        return self.edge_fc2(edge_features).squeeze(-1)  # Single logit per edge\n",
    "\n",
    "\n",
    "# Example initialization\n",
    "model = EdgeClassifierWithEdgeFeatures(node_in_channels=3, edge_in_channels=4, hidden_channels=16)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_model(model, data_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.edge_attr)  # Forward pass\n",
    "            loss = criterion(out, data.y.float())  # Convert target to float\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimizer step\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_idx = int(split_ratio * len(processed_dataset))\n",
    "train_dataset = processed_dataset[:split_idx]\n",
    "test_dataset = processed_dataset[split_idx:]\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust batch size based on your resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of testing graphs: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 30  # Adjust as needed\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test/validation data loader and compute metrics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data in data_loader:\n",
    "            # Move data to device (CPU/GPU)\n",
    "            inputs = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            edge_attr = data.edge_attr.to(device)  # Edge features\n",
    "            labels = data.y.to(device)  # Edge labels\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs, edge_index, edge_attr)\n",
    "            predicted = torch.sigmoid(outputs) > 0.5  # Convert logits to binary predictions\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Example Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Evaluating the model on the test dataset...\")\n",
    "accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_bce_model(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            logits = model(data.x, data.edge_index, data.edge_attr)\n",
    "            probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "            preds = (probs >= threshold).long()  # Apply threshold\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(data.y)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).cpu().numpy()\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(all_targets, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluate_bce_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_with_thresholds(model, data_loader, thresholds):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr)  # Forward pass\n",
    "            probs = torch.softmax(out, dim=1)[:, 1]  # Probability for class 1\n",
    "            all_probs.append(probs)\n",
    "            all_targets.append(data.y)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_probs = torch.cat(all_probs, dim=0).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).cpu().numpy()\n",
    "\n",
    "    # Evaluate at different thresholds\n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        preds = (all_probs >= threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_targets, preds, average='binary', zero_division=0\n",
    "        )\n",
    "        results.append((threshold, precision, recall, f1))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define thresholds to test\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "# Evaluate model with varying thresholds\n",
    "results = evaluate_with_thresholds(model, test_loader, thresholds)\n",
    "\n",
    "# Print results\n",
    "print(\"Threshold | Precision | Recall | F1-Score\")\n",
    "for threshold, precision, recall, f1 in results:\n",
    "    print(f\"  {threshold:.2f}    |  {precision:.4f}  |  {recall:.4f} |  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr)  # Forward pass\n",
    "            preds = out.argmax(dim=1)  # Predicted class\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(data.y)\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).cpu().numpy()\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(all_targets, all_preds, digits=4))\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for addressing class imbalance.\n",
    "        Args:\n",
    "            alpha: Weighting factor for classes (tensor or list). Default is None (no weighting).\n",
    "            gamma: Focusing parameter to reduce the impact of easy examples.\n",
    "            reduction: Reduction method for loss ('mean', 'sum', 'none').\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor(alpha, dtype=torch.float) if alpha is not None else None\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of the correct class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        # Apply alpha weighting if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha_factor = self.alpha[targets]\n",
    "            focal_loss = alpha_factor * focal_loss\n",
    "\n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Identify graphs containing the minority class (label 1)\n",
    "def find_minority_graphs(dataset):\n",
    "    minority_graph_indices = []\n",
    "    for idx, data in enumerate(dataset):\n",
    "        if 1 in data.y:  # Check if the graph contains the minority class\n",
    "            minority_graph_indices.append(idx)\n",
    "    return minority_graph_indices\n",
    "\n",
    "minority_graphs = find_minority_graphs(train_dataset)\n",
    "print(f\"Number of graphs with minority class: {len(minority_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom DataLoader with oversampling\n",
    "def oversampling_dataloader(dataset, minority_graphs, oversample_factor=3, batch_size=32):\n",
    "    all_indices = list(range(len(dataset)))\n",
    "    oversampled_indices = all_indices + minority_graphs * oversample_factor\n",
    "\n",
    "    # Shuffle the oversampled indices\n",
    "    random.shuffle(oversampled_indices)\n",
    "\n",
    "    # Create a DataLoader with the oversampled indices\n",
    "    return DataLoader(\n",
    "        [dataset[i] for i in oversampled_indices],  # Oversampled dataset\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # No need to shuffle, as indices are already randomized\n",
    "    )\n",
    "\n",
    "# Create the oversampling DataLoader\n",
    "train_loader = oversampling_dataloader(train_dataset, minority_graphs, oversample_factor=3, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model\n",
    "num_epochs = 20  # Adjust as needed\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GATEdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, heads=2):\n",
    "        super(GATEdgeClassifier, self).__init__()\n",
    "        self.gat1 = GATConv(node_in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=False)\n",
    "\n",
    "        self.edge_fc1 = torch.nn.Linear(hidden_channels * 2 + edge_in_channels, hidden_channels)\n",
    "        self.edge_fc2 = torch.nn.Linear(hidden_channels, 1)  # Single output for BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "\n",
    "        src, dst = edge_index\n",
    "        edge_node_features = torch.cat([x[src], x[dst]], dim=-1)\n",
    "        edge_features = torch.cat([edge_node_features, edge_attr], dim=-1)\n",
    "\n",
    "        edge_features = F.relu(self.edge_fc1(edge_features))\n",
    "        return self.edge_fc2(edge_features).squeeze(-1)  # Output logits as 1D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GAT model\n",
    "#model = GATEdgeClassifier(node_in_channels=3, edge_in_channels=4, hidden_channels=16, heads=2)\n",
    "model = GATEdgeClassifier(node_in_channels=3, edge_in_channels=4, hidden_channels=32, heads=4)\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAT model\n",
    "num_epochs = 30  # Adjust as needed\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned GAT model\n",
    "evaluate_bce_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "class EdgeGAT(torch.nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, heads=4):\n",
    "        super(EdgeGAT, self).__init__()\n",
    "        self.gat1 = GATConv(node_in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=False)\n",
    "\n",
    "        # Edge-specific fully connected layers\n",
    "        self.edge_fc1 = Linear(hidden_channels * 2 + edge_in_channels, hidden_channels)\n",
    "        self.edge_fc2 = Linear(hidden_channels, 1)  # Single output for BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Node embeddings via GAT layers\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "\n",
    "        # Prepare edge embeddings\n",
    "        src, dst = edge_index\n",
    "        edge_node_features = torch.cat([x[src], x[dst]], dim=-1)\n",
    "        edge_features = torch.cat([edge_node_features, edge_attr], dim=-1)\n",
    "\n",
    "        # Pass through edge-specific layers\n",
    "        edge_features = F.relu(self.edge_fc1(edge_features))\n",
    "        return self.edge_fc2(edge_features).squeeze(-1)  # Single logit per edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Edge-GAT model\n",
    "model = EdgeGAT(node_in_channels=3, edge_in_channels=4, hidden_channels=32, heads=4)\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Edge-GAT model\n",
    "num_epochs = 15  # Adjust as needed\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Edge-GAT model\n",
    "evaluate_bce_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_oversampling_loader(dataset, batch_size=32, oversample_factor=3):\n",
    "    \"\"\"\n",
    "    Create a DataLoader with dynamic edge-level oversampling.\n",
    "    Args:\n",
    "        dataset: List of Data objects.\n",
    "        batch_size: Number of graphs per batch.\n",
    "        oversample_factor: Multiplier for minority-class edges.\n",
    "    Returns:\n",
    "        DataLoader with oversampled edges.\n",
    "    \"\"\"\n",
    "    oversampled_data = []\n",
    "\n",
    "    for data in dataset:\n",
    "        # Separate edges by class\n",
    "        minority_edges = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        majority_edges = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Oversample minority edges\n",
    "        if len(minority_edges) > 0:  # Avoid empty minority class\n",
    "            oversampled_minority_edges = minority_edges.repeat(oversample_factor)\n",
    "            oversampled_edges = torch.cat([majority_edges, oversampled_minority_edges])\n",
    "\n",
    "            # Shuffle edges\n",
    "            shuffled_indices = torch.randperm(len(oversampled_edges))\n",
    "            oversampled_edges = oversampled_edges[shuffled_indices]\n",
    "\n",
    "            # Update edge attributes and labels\n",
    "            data.edge_index = data.edge_index[:, oversampled_edges]\n",
    "            data.edge_attr = data.edge_attr[oversampled_edges]\n",
    "            data.y = data.y[oversampled_edges]\n",
    "\n",
    "        oversampled_data.append(data)\n",
    "\n",
    "    return DataLoader(oversampled_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader with edge-level oversampling\n",
    "train_loader = create_edge_oversampling_loader(train_dataset, batch_size=32, oversample_factor=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Edge-GAT model with oversampled edges\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Edge-GAT model with oversampled edges\n",
    "evaluate_bce_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class ClassBalancedFocalLoss(torch.nn.Module):\n",
    "    def __init__(self, beta=0.9999, gamma=2.0):\n",
    "        \"\"\"\n",
    "        Class-Balanced Focal Loss for handling severe class imbalance.\n",
    "        Args:\n",
    "            beta: Hyperparameter to control class weighting (near 1.0 for large datasets).\n",
    "            gamma: Focusing parameter to adjust the contribution of easy and hard examples.\n",
    "        \"\"\"\n",
    "        super(ClassBalancedFocalLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Logits from the model.\n",
    "            targets: Ground truth labels (0 or 1).\n",
    "        Returns:\n",
    "            Loss value computed for the batch.\n",
    "        \"\"\"\n",
    "        # Compute effective number of samples\n",
    "        num_samples = targets.size(0)\n",
    "        class_counts = torch.bincount(targets.long(), minlength=2)\n",
    "        effective_num = (1.0 - self.beta) / (1.0 - self.beta**class_counts.float())\n",
    "\n",
    "        # Class weights\n",
    "        class_weights = effective_num / effective_num.sum()\n",
    "\n",
    "        # Logits to probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        probs = probs * targets + (1 - probs) * (1 - targets)  # Adjust for binary case\n",
    "        focal_weights = (1 - probs) ** self.gamma\n",
    "\n",
    "        # Apply class weights\n",
    "        class_weight = class_weights[targets.long()]\n",
    "        loss = -class_weight * focal_weights * torch.log(probs + 1e-8)\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class-balanced focal loss\n",
    "criterion = ClassBalancedFocalLoss(beta=0.9999, gamma=2.0)\n",
    "print(\"Using Class-Balanced Focal Loss with beta=0.9999 and gamma=2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with Class-Balanced Focal Loss\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Edge-GAT model with Class-Balanced Focal Loss\n",
    "evaluate_bce_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "def evaluate_model_with_metrics(model, data_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            logits = model(data.x, data.edge_index, data.edge_attr)\n",
    "            probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "            preds = (probs >= threshold).long()  # Apply threshold\n",
    "            all_preds.append(preds)\n",
    "            all_logits.append(probs)\n",
    "            all_targets.append(data.y)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).cpu().numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).cpu().numpy()\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\\n\", classification_report(all_targets, all_preds, digits=4))\n",
    "\n",
    "    # Additional Metrics\n",
    "    auc = roc_auc_score(all_targets, all_logits)\n",
    "    balanced_acc = balanced_accuracy_score(all_targets, all_preds)\n",
    "    f1_class_1 = f1_score(all_targets, all_preds, pos_label=1)\n",
    "\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"F1-Score for Class 1: {f1_class_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with new metrics\n",
    "evaluate_model_with_metrics(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_to_edges_safe(dataset):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to oversample the minority class at the edge level\n",
    "    while maintaining node index consistency.\n",
    "    Args:\n",
    "        dataset: List of torch_geometric.data.Data objects.\n",
    "    Returns:\n",
    "        A new dataset with SMOTE-applied edge-level augmentation.\n",
    "    \"\"\"\n",
    "    augmented_dataset = []\n",
    "\n",
    "    for data in dataset:\n",
    "        edge_features = data.edge_attr.cpu().numpy()\n",
    "        edge_labels = data.y.cpu().numpy()\n",
    "\n",
    "        # Apply SMOTE only if the minority class is present\n",
    "        if len(set(edge_labels)) > 1:\n",
    "            smote = SMOTE(k_neighbors=5)\n",
    "            edge_features_smote, edge_labels_smote = smote.fit_resample(edge_features, edge_labels)\n",
    "\n",
    "            # Update edge attributes and labels\n",
    "            data.edge_attr = torch.tensor(edge_features_smote, dtype=torch.float)\n",
    "            data.y = torch.tensor(edge_labels_smote, dtype=torch.long)\n",
    "\n",
    "            # Limit edge_index to the original number of nodes\n",
    "            num_edges = data.edge_attr.size(0)\n",
    "            edge_index = torch.randint(0, data.x.size(0), (2, num_edges))  # Randomly assign edges\n",
    "            data.edge_index = edge_index\n",
    "\n",
    "        augmented_dataset.append(data)\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "# Apply the updated SMOTE function to the training dataset\n",
    "train_dataset_smote = apply_smote_to_edges_safe(train_dataset)\n",
    "\n",
    "# Create a DataLoader for the SMOTE-augmented dataset\n",
    "train_loader_smote = DataLoader(train_dataset_smote, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Edge-GAT model with corrected SMOTE-augmented dataset\n",
    "train_model(model, train_loader_smote, criterion, optimizer, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Edge-GAT model\n",
    "evaluate_model_with_metrics(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
