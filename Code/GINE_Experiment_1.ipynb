{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### New Experiment Adaeze Experiment_1 \n",
    "# Cell 1: Basic Imports and Data Loading\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GINEConv, GATConv, GraphConv\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'dataset/ieee24/ieee24/processed_r/data.pt'\n",
    "loaded_data = torch.load(file_path)\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Loaded data type:\", type(loaded_data))\n",
    "print(\"Length of dataset tuple:\", len(loaded_data))\n",
    "\n",
    "# Inspect first element (summary only)\n",
    "print(\"\\nFirst element type:\", type(loaded_data[0]))\n",
    "\n",
    "# Extract metadata dictionary\n",
    "metadata_dict = loaded_data[1]\n",
    "print(\"\\nMetadata keys:\", list(metadata_dict.keys()))\n",
    "\n",
    "# Preview metadata values (first 10 entries)\n",
    "for key, val in metadata_dict.items():\n",
    "    preview = val[:10] if hasattr(val, '__len__') else \"N/A\"\n",
    "    print(f\"{key}: {type(val)}, first 10: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Add strict validation\n",
    "\n",
    "\n",
    "def get_subgraph(data_flat, meta_dict, i):\n",
    "    \"\"\"\n",
    "    data_flat: The big flattened Data object (loaded_data[0])\n",
    "    meta_dict: The dictionary of offsets (loaded_data[1])\n",
    "    i        : Index of the subgraph we want to reconstruct\n",
    "\n",
    "    returns: a PyG Data object representing the i-th subgraph\n",
    "    \"\"\"\n",
    "    # 1) Node offsets\n",
    "    x_start = meta_dict['x'][i].item()\n",
    "    x_end   = meta_dict['x'][i+1].item()\n",
    "    x_i = data_flat.x[x_start:x_end]\n",
    "    \n",
    "    # 2) Edge offsets\n",
    "    e_start = meta_dict['edge_index'][i].item()\n",
    "    e_end   = meta_dict['edge_index'][i+1].item()\n",
    "    edge_index_i = data_flat.edge_index[:, e_start:e_end]\n",
    "    edge_attr_i  = data_flat.edge_attr[e_start:e_end]\n",
    "    \n",
    "    # 3) Load TRUE binary edge labels (explanation_mask)\n",
    "    # --------------------------------------------------\n",
    "    edge_mask_i = data_flat.edge_mask[e_start:e_end].float()  # Convert to float\n",
    "    \n",
    "    # NEW: Strict validation\n",
    "    if not torch.all(torch.isin(edge_mask_i, torch.tensor([0., 1.]))):\n",
    "        print(f\"BAD SUBGRAPH {i}:\")\n",
    "        print(\"Unique values:\", edge_mask_i.unique())\n",
    "        print(\"Edge indices:\", edge_index_i)\n",
    "        raise ValueError(\"Edge mask contains non-binary values\")\n",
    "    \n",
    "    # 4) Graph label (binary or multi-class)\n",
    "    y_i = data_flat.y[i]\n",
    "    \n",
    "    # 5) Build a new Data object\n",
    "    subgraph_i = Data(\n",
    "        x=x_i,\n",
    "        edge_index=edge_index_i,\n",
    "        edge_attr=edge_attr_i,\n",
    "        y=y_i.unsqueeze(0),  # Keep graph-level label if needed\n",
    "        edge_mask=edge_mask_i  # Add binary edge labels\n",
    "    )\n",
    "    \n",
    "    return subgraph_i\n",
    "\n",
    "# Test subgraph reconstruction\n",
    "i_test = 0\n",
    "subgraph_0 = get_subgraph(loaded_data[0], loaded_data[1], i_test)\n",
    "print(\"Subgraph 0:\")\n",
    "print(subgraph_0)\n",
    "print(\"Edge mask values:\", subgraph_0.edge_mask.unique())  # Should be [0., 1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3: Verification \n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "dataset = [\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 1, 0]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3),  # Category B or D\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 0, 1]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3)   # Category B or D\n",
    "]\n",
    "\n",
    "def verify_edge_mask_coverage(dataset):\n",
    "    \"\"\"Check if edge_mask is defined for all graphs with cascading failures (y=1).\"\"\"\n",
    "    print(\"Verifying Edge Mask Coverage\")\n",
    "    has_cascading = 0\n",
    "    has_edge_mask_defined = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        if graph.y.item() == 1:  # Graphs with cascading failures (Categories A and C)\n",
    "            has_cascading += 1\n",
    "            if graph.edge_mask is not None and len(graph.edge_mask) == graph.num_edges:\n",
    "                has_edge_mask_defined += 1\n",
    "            else:\n",
    "                print(f\"Graph {i}: Missing or incomplete edge_mask for cascading failure graph.\")\n",
    "    \n",
    "    print(f\"Graphs with cascading failures: {has_cascading}\")\n",
    "    print(f\"Graphs with defined edge_mask: {has_edge_mask_defined}\")\n",
    "    if has_cascading == has_edge_mask_defined:\n",
    "        print(\" Edge mask coverage is complete for cascading failure graphs.\")\n",
    "    else:\n",
    "        print(\"Edge mask is missing or incomplete for some cascading failure graphs.\")\n",
    "\n",
    "def check_edge_label_distribution(dataset):\n",
    "    \"\"\"Examine the distribution of edge labels (1s and 0s) across graphs.\"\"\"\n",
    "    print(\"\\n Checking Edge Label Distribution\")\n",
    "    total_edges = 0\n",
    "    tripped_edges = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        num_tripped = edge_mask.sum().item()\n",
    "        total_edges += len(edge_mask)\n",
    "        tripped_edges += num_tripped\n",
    "        print(f\"Graph {i}: {num_tripped} tripped edges (1s), {len(edge_mask) - num_tripped} non-tripped (0s)\")\n",
    "    \n",
    "    print(f\"Total edges: {total_edges}\")\n",
    "    print(f\"Tripped edges (1s): {tripped_edges}\")\n",
    "    print(f\"Non-tripped edges (0s): {total_edges - tripped_edges}\")\n",
    "    print(f\"Percentage of tripped edges: {(tripped_edges / total_edges * 100):.2f}%\")\n",
    "\n",
    "def validate_graph_edge_consistency(dataset):\n",
    "    \"\"\"Ensure edge_mask aligns with graph-level labels (y).\"\"\"\n",
    "    print(\"\\n Validating Graph-Edge Label Consistency \")\n",
    "    all_valid = True\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        y = graph.y.item()\n",
    "        \n",
    "        if y == 1:  # Categories A and C (cascading failures)\n",
    "            if edge_mask.sum() == 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=1 but no tripped edges in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=1 and tripped edges present.\")\n",
    "        elif y == 0:  # Categories B and D (no cascading failures)\n",
    "            if edge_mask.sum() > 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=0 but tripped edges present in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=0 and no tripped edges.\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"All graphs have consistent edge_mask and y labels.\")\n",
    "    else:\n",
    "        print(\" Some graphs have inconsistencies between edge_mask and y.\")\n",
    "\n",
    "# Run the verifications\n",
    "verify_edge_mask_coverage(dataset)\n",
    "check_edge_label_distribution(dataset)\n",
    "validate_graph_edge_consistency(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create a PyTorch Dataset for our subgraphs\n",
    "\n",
    "class PowerGraphDataset(Dataset):\n",
    "    def __init__(self, data_flat, meta_dict, indices=None, filter_category_A=True):\n",
    "        \"\"\"\n",
    "        data_flat:  The giant flattened Data object\n",
    "        meta_dict:  Dictionary of offsets\n",
    "        indices:    Subgraph indices to include\n",
    "        filter_category_A: If True, only include graphs with cascading failures (edge_mask != 0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_flat = data_flat\n",
    "        self.meta_dict = meta_dict\n",
    "        self.filter_category_A = filter_category_A\n",
    "        \n",
    "        if indices is None:\n",
    "            # Default to all graphs (0 to num_subgraphs-1)\n",
    "            self.indices = range(len(meta_dict['x']) - 1)\n",
    "        else:\n",
    "            self.indices = indices\n",
    "        \n",
    "        # Filter to Category A (DNS > 0 with cascading failures)\n",
    "        if self.filter_category_A:\n",
    "            self.indices = self._filter_category_A()\n",
    "    \n",
    "    def _filter_category_A(self):\n",
    "        \"\"\"Retain indices where edge_mask has at least one failure (1)\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in self.indices:\n",
    "            e_start = self.meta_dict['edge_index'][idx].item()\n",
    "            e_end = self.meta_dict['edge_index'][idx+1].item()\n",
    "            edge_mask = self.data_flat.edge_mask[e_start:e_end]  # Use edge_mask\n",
    "            if edge_mask.sum() > 0:  # At least one failed edge\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subgraph_id = self.indices[idx]\n",
    "        return get_subgraph(self.data_flat, self.meta_dict, subgraph_id)\n",
    "\n",
    "# Create dataset (only Category A graphs)\n",
    "full_dataset = PowerGraphDataset(loaded_data[0], loaded_data[1], filter_category_A=True)\n",
    "print(\"Total subgraphs in full_dataset:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating full_dataset (Cell 5):\n",
    "all_edge_masks = torch.cat([batch.edge_mask for batch in full_dataset])\n",
    "num_positive = all_edge_masks.sum().item()\n",
    "num_negative = len(all_edge_masks) - num_positive\n",
    "\n",
    "print(f\"Edge label distribution:\")\n",
    "print(f\"- Failed edges (1): {num_positive} ({num_positive / len(all_edge_masks):.2%})\")\n",
    "print(f\"- Stable edges (0): {num_negative} ({num_negative / len(all_edge_masks):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train/Val/Test split & DataLoaders (with class-aware splitting)\n",
    "\n",
    "# 1) Handle extreme class imbalance (3.25% positive edges)\n",
    "# --------------------------------------------------------\n",
    "# Calculate split sizes based on the filtered Category A dataset\n",
    "num_subgraphs = len(full_dataset)  # 3444 (from your output)\n",
    "train_size = int(0.8 * num_subgraphs)   # ~2755\n",
    "val_size = int(0.1 * num_subgraphs)     # ~344\n",
    "test_size = num_subgraphs - train_size - val_size  # ~345\n",
    "\n",
    "# 2) Stratified split to preserve class distribution\n",
    "# (PyTorch's random_split doesn't stratify, so we use a custom approach)\n",
    "indices = np.arange(num_subgraphs)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx = indices[train_size:train_size+val_size]\n",
    "test_idx = indices[train_size+val_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_idx)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Val set size:   {len(val_dataset)}\")\n",
    "print(f\"Test set size:  {len(test_dataset)}\")\n",
    "\n",
    "# 3) Build PyG DataLoaders with class-aware sampling\n",
    "batch_size = 32\n",
    "\n",
    "# Use weighted sampler to handle edge-level imbalance\n",
    "graph_weights = [batch.edge_mask.float().mean().item() for batch in full_dataset]  # Proportion of positive edges per graph\n",
    "train_sample_weights = [graph_weights[i] for i in train_idx]\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    train_sample_weights, len(train_idx), replacement=True\n",
    ")\n",
    "\n",
    "train_loader = PyGDataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with batch_size =\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_dim  # Add this line to expose input dimension\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        # final layer\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "\n",
    "class EdgeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN that:\n",
    "      1. Takes node features x and edge features edge_attr.\n",
    "      2. Produces node embeddings via GINEConv.\n",
    "      3. For each edge (u,v), we build an 'edge embedding' = cat([h_u, h_v, edge_attr_uv])\n",
    "         and pass it to a small MLP to get a prediction logit.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_node=3,     # Node feature dimension (e.g., 3 in the dataset)\n",
    "        in_channels_edge=4,     # Edge feature dimension (e.g., 4 in the dataset)\n",
    "        hidden_dim=32,\n",
    "        num_layers=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1) We'll do an initial linear projection: node features (3) -> (hidden_dim)\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        \n",
    "        # 2) The GNN \"message\" MLP used inside GINEConv\n",
    "        self.gnn_mlp = MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim)\n",
    "        \n",
    "        # 3) Our GINEConv layer (Fix: Removed in_channels argument)\n",
    "        self.conv = GINEConv(\n",
    "            nn=self.gnn_mlp,\n",
    "            edge_dim=in_channels_edge  # Ensure edge_dim matches input edge features\n",
    "        )\n",
    "        \n",
    "        # 4) A small MLP to classify edges from [h_u, h_v, edge_attr] -> logit\n",
    "        #    So input_dim = hidden_dim + hidden_dim + in_channels_edge\n",
    "        edge_in_dim = hidden_dim * 2 + in_channels_edge\n",
    "        self.edge_mlp = MLP(edge_in_dim, hidden_dim, 1, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        x          : Node features of shape [num_nodes, in_channels_node]\n",
    "        edge_index : [2, num_edges]\n",
    "        edge_attr  : Edge features of shape [num_edges, in_channels_edge]\n",
    "        \n",
    "        returns:\n",
    "            edge_logits: shape [num_edges], raw scores for 'edge fails or not'\n",
    "        \"\"\"\n",
    "        # 1) Project node features to hidden dimension\n",
    "        h = self.fc_in(x)  # [num_nodes, hidden_dim]\n",
    "\n",
    "        # 2) GINEConv layer\n",
    "        h = self.conv(h, edge_index, edge_attr)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # 3) Build edge embeddings for each edge\n",
    "        h_u = h[edge_index[0]]  # [num_edges, hidden_dim]\n",
    "        h_v = h[edge_index[1]]  # [num_edges, hidden_dim]\n",
    "        edge_feats = torch.cat([h_u, h_v, edge_attr], dim=1)\n",
    "        \n",
    "        # 4) Predict logit for each edge\n",
    "        edge_logits = self.edge_mlp(edge_feats).squeeze(dim=-1)  # shape [num_edges]\n",
    "        \n",
    "        return edge_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Loop for Edge Classification\n",
    "\n",
    "device = torch.device(\"cpu\")  # Force CPU for debugging\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for positive class\n",
    "        self.gamma = gamma  # Focuses on hard examples\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)  # pt = p if label=1, 1-p otherwise\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * bce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "# 2) Instantiate our model\n",
    "model = EdgeClassifier(\n",
    "    in_channels_node=3,\n",
    "    in_channels_edge=4,\n",
    "    hidden_dim=32,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# 3) Define loss with class weighting\n",
    "pos_weight_value = min(245398.0 / 8252.0, 100.0)  # Cap at 100:1 ratio\n",
    "pos_weight = torch.tensor([pos_weight_value]).to(device)\n",
    "#criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)  # alpha >0.5 to emphasize positives\n",
    "\n",
    "# Define the optimizer before training**\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 4) Helper function to train for 1 epoch\n",
    "def train_one_epoch(model, loader, device, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        edge_labels = batch.edge_mask.float()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, edge_labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check for NaN/Infs in gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\" NaN detected in gradients of {name}\")\n",
    "                raise RuntimeError(\"NaN detected in gradients\")\n",
    "            if param.grad is not None and torch.isinf(param.grad).any():\n",
    "                print(f\" Inf detected in gradients of {name}\")\n",
    "                raise RuntimeError(\"Inf detected in gradients\")\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 5) Enhanced evaluation metrics\n",
    "def calculate_metrics(preds, labels):\n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    precision = TP / (TP + FP + 1e-8)  # Avoid division by zero\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# 6) Modified evaluate function\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "    total_edges = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_labels = batch.edge_mask.float()\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(logits, edge_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Metrics\n",
    "            preds = (torch.sigmoid(logits) > 0.4).long()\n",
    "            prec, rec, f1 = calculate_metrics(preds, edge_labels.long())\n",
    "            \n",
    "            metrics['precision'] += prec * edge_labels.numel()\n",
    "            metrics['recall'] += rec * edge_labels.numel()\n",
    "            metrics['f1'] += f1 * edge_labels.numel()\n",
    "            total_edges += edge_labels.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    for key in metrics:\n",
    "        metrics[key] /= total_edges\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "# 7) Training loop with improved logging\n",
    "num_epochs = 5\n",
    "\n",
    "# Ensure `train_loader` and `val_loader` exist\n",
    "try:\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, device, optimizer)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"  Val Recall:    {val_metrics['recall']:.4f}\")\n",
    "        print(f\"  Val F1:        {val_metrics['f1']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "except NameError as e:\n",
    "    print(f\" ERROR: Missing DataLoader! {e}\")\n",
    "    print(\"Make sure train_loader and val_loader are defined before running the training loop.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
