{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.168027.bright04/ipykernel_3207062/1793163579.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(file_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'tuple'>\n",
      "Length of dataset tuple: 2\n",
      "\n",
      "First element type: <class 'torch_geometric.data.data.Data'>\n",
      "\n",
      "Metadata keys: ['x', 'edge_index', 'edge_attr', 'y', 'edge_mask', 'idx']\n",
      "x: <class 'torch.Tensor'>, first 10: tensor([   0,  118,  236,  354,  472,  590,  708,  826,  944, 1062])\n",
      "edge_index: <class 'torch.Tensor'>, first 10: tensor([   0,  370,  740, 1110, 1480, 1850, 2220, 2590, 2960, 3330])\n",
      "edge_attr: <class 'torch.Tensor'>, first 10: tensor([   0,  370,  740, 1110, 1480, 1850, 2220, 2590, 2960, 3330])\n",
      "y: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "edge_mask: <class 'torch.Tensor'>, first 10: tensor([   0,  370,  740, 1110, 1480, 1850, 2220, 2590, 2960, 3330])\n",
      "idx: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Basic Imports and Data Loading\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GINEConv, GATConv, GraphConv\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "#file_path = 'dataset/ieee24/ieee24/processed_b/data.pt'\n",
    "#file_path = 'dataset/ieee39/processed_b/data.pt'\n",
    "#file_path = 'dataset/uk/processed_b/data.pt'\n",
    "file_path = 'dataset/ieee118/processed_b/data.pt'\n",
    "\n",
    "loaded_data = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Loaded data type:\", type(loaded_data))\n",
    "print(\"Length of dataset tuple:\", len(loaded_data))\n",
    "\n",
    "# Inspect first element (summary only)\n",
    "print(\"\\nFirst element type:\", type(loaded_data[0]))\n",
    "\n",
    "# Extract metadata dictionary\n",
    "metadata_dict = loaded_data[1]\n",
    "print(\"\\nMetadata keys:\", list(metadata_dict.keys()))\n",
    "\n",
    "# Preview metadata values (first 10 entries)\n",
    "for key, val in metadata_dict.items():\n",
    "    preview = val[:10] if hasattr(val, '__len__') else \"N/A\"\n",
    "    print(f\"{key}: {type(val)}, first 10: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph 0:\n",
      "Data(x=[118, 3], edge_index=[2, 370], edge_attr=[370, 4], y=[1, 1], edge_mask=[370])\n",
      "Edge mask values: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Add strict validation\n",
    "\n",
    "\n",
    "def get_subgraph(data_flat, meta_dict, i):\n",
    "    \"\"\"\n",
    "    data_flat: The big flattened Data object (loaded_data[0])\n",
    "    meta_dict: The dictionary of offsets (loaded_data[1])\n",
    "    i        : Index of the subgraph we want to reconstruct\n",
    "\n",
    "    returns: a PyG Data object representing the i-th subgraph\n",
    "    \"\"\"\n",
    "    # 1) Node offsets\n",
    "    x_start = meta_dict['x'][i].item()\n",
    "    x_end   = meta_dict['x'][i+1].item()\n",
    "    x_i = data_flat.x[x_start:x_end]\n",
    "    \n",
    "    # 2) Edge offsets\n",
    "    e_start = meta_dict['edge_index'][i].item()\n",
    "    e_end   = meta_dict['edge_index'][i+1].item()\n",
    "    edge_index_i = data_flat.edge_index[:, e_start:e_end]\n",
    "    edge_attr_i  = data_flat.edge_attr[e_start:e_end]\n",
    "    \n",
    "    # 3) Load TRUE binary edge labels (explanation_mask)\n",
    "    # --------------------------------------------------\n",
    "    edge_mask_i = data_flat.edge_mask[e_start:e_end].float()  # Convert to float\n",
    "    \n",
    "    # NEW: Strict validation\n",
    "    if not torch.all(torch.isin(edge_mask_i, torch.tensor([0., 1.]))):\n",
    "        print(f\"BAD SUBGRAPH {i}:\")\n",
    "        print(\"Unique values:\", edge_mask_i.unique())\n",
    "        print(\"Edge indices:\", edge_index_i)\n",
    "        raise ValueError(\"Edge mask contains non-binary values\")\n",
    "    \n",
    "    # 4) Graph label (binary or multi-class)\n",
    "    y_i = data_flat.y[i]\n",
    "    \n",
    "    # 5) Build a new Data object\n",
    "    subgraph_i = Data(\n",
    "        x=x_i,\n",
    "        edge_index=edge_index_i,\n",
    "        edge_attr=edge_attr_i,\n",
    "        y=y_i.unsqueeze(0),  # Keep graph-level label if needed\n",
    "        edge_mask=edge_mask_i  # Add binary edge labels\n",
    "    )\n",
    "    \n",
    "    return subgraph_i\n",
    "\n",
    "# Test subgraph reconstruction\n",
    "i_test = 0\n",
    "subgraph_0 = get_subgraph(loaded_data[0], loaded_data[1], i_test)\n",
    "print(\"Subgraph 0:\")\n",
    "print(subgraph_0)\n",
    "print(\"Edge mask values:\", subgraph_0.edge_mask.unique())  # Should be [0., 1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Edge Mask Coverage\n",
      "Graphs with cascading failures: 2\n",
      "Graphs with defined edge_mask: 2\n",
      " Edge mask coverage is complete for cascading failure graphs.\n",
      "\n",
      " Checking Edge Label Distribution\n",
      "Graph 0: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 1: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Graph 2: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 3: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Total edges: 12\n",
      "Tripped edges (1s): 2\n",
      "Non-tripped edges (0s): 10\n",
      "Percentage of tripped edges: 16.67%\n",
      "\n",
      " Validating Graph-Edge Label Consistency \n",
      "Graph 0: Consistent - y=1 and tripped edges present.\n",
      "Graph 1: Consistent - y=0 and no tripped edges.\n",
      "Graph 2: Consistent - y=1 and tripped edges present.\n",
      "Graph 3: Consistent - y=0 and no tripped edges.\n",
      "All graphs have consistent edge_mask and y labels.\n"
     ]
    }
   ],
   "source": [
    "#Cell 3: Verification \n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "dataset = [\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 1, 0]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3),  # Category B or D\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 0, 1]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3)   # Category B or D\n",
    "]\n",
    "\n",
    "def verify_edge_mask_coverage(dataset):\n",
    "    \"\"\"Check if edge_mask is defined for all graphs with cascading failures (y=1).\"\"\"\n",
    "    print(\"Verifying Edge Mask Coverage\")\n",
    "    has_cascading = 0\n",
    "    has_edge_mask_defined = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        if graph.y.item() == 1:  # Graphs with cascading failures (Categories A and C)\n",
    "            has_cascading += 1\n",
    "            if graph.edge_mask is not None and len(graph.edge_mask) == graph.num_edges:\n",
    "                has_edge_mask_defined += 1\n",
    "            else:\n",
    "                print(f\"Graph {i}: Missing or incomplete edge_mask for cascading failure graph.\")\n",
    "    \n",
    "    print(f\"Graphs with cascading failures: {has_cascading}\")\n",
    "    print(f\"Graphs with defined edge_mask: {has_edge_mask_defined}\")\n",
    "    if has_cascading == has_edge_mask_defined:\n",
    "        print(\" Edge mask coverage is complete for cascading failure graphs.\")\n",
    "    else:\n",
    "        print(\"Edge mask is missing or incomplete for some cascading failure graphs.\")\n",
    "\n",
    "def check_edge_label_distribution(dataset):\n",
    "    \"\"\"Examine the distribution of edge labels (1s and 0s) across graphs.\"\"\"\n",
    "    print(\"\\n Checking Edge Label Distribution\")\n",
    "    total_edges = 0\n",
    "    tripped_edges = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        num_tripped = edge_mask.sum().item()\n",
    "        total_edges += len(edge_mask)\n",
    "        tripped_edges += num_tripped\n",
    "        print(f\"Graph {i}: {num_tripped} tripped edges (1s), {len(edge_mask) - num_tripped} non-tripped (0s)\")\n",
    "    \n",
    "    print(f\"Total edges: {total_edges}\")\n",
    "    print(f\"Tripped edges (1s): {tripped_edges}\")\n",
    "    print(f\"Non-tripped edges (0s): {total_edges - tripped_edges}\")\n",
    "    print(f\"Percentage of tripped edges: {(tripped_edges / total_edges * 100):.2f}%\")\n",
    "\n",
    "def validate_graph_edge_consistency(dataset):\n",
    "    \"\"\"Ensure edge_mask aligns with graph-level labels (y).\"\"\"\n",
    "    print(\"\\n Validating Graph-Edge Label Consistency \")\n",
    "    all_valid = True\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        y = graph.y.item()\n",
    "        \n",
    "        if y == 1:  # Categories A and C (cascading failures)\n",
    "            if edge_mask.sum() == 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=1 but no tripped edges in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=1 and tripped edges present.\")\n",
    "        elif y == 0:  # Categories B and D (no cascading failures)\n",
    "            if edge_mask.sum() > 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=0 but tripped edges present in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=0 and no tripped edges.\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"All graphs have consistent edge_mask and y labels.\")\n",
    "    else:\n",
    "        print(\" Some graphs have inconsistencies between edge_mask and y.\")\n",
    "\n",
    "# Run the verifications\n",
    "verify_edge_mask_coverage(dataset)\n",
    "check_edge_label_distribution(dataset)\n",
    "validate_graph_edge_consistency(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subgraphs in full_dataset: 1167\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create a PyTorch Dataset for our subgraphs\n",
    "\n",
    "class PowerGraphDataset(Dataset):\n",
    "    def __init__(self, data_flat, meta_dict, indices=None, filter_category_A=True):\n",
    "        \"\"\"\n",
    "        data_flat:  The giant flattened Data object\n",
    "        meta_dict:  Dictionary of offsets\n",
    "        indices:    Subgraph indices to include\n",
    "        filter_category_A: If True, only include graphs with cascading failures (edge_mask != 0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_flat = data_flat\n",
    "        self.meta_dict = meta_dict\n",
    "        self.filter_category_A = filter_category_A\n",
    "        \n",
    "        if indices is None:\n",
    "            # Default to all graphs (0 to num_subgraphs-1)\n",
    "            self.indices = range(len(meta_dict['x']) - 1)\n",
    "        else:\n",
    "            self.indices = indices\n",
    "        \n",
    "        # Filter to Category A (DNS > 0 with cascading failures)\n",
    "        if self.filter_category_A:\n",
    "            self.indices = self._filter_category_A()\n",
    "    \n",
    "    def _filter_category_A(self):\n",
    "        \"\"\"Retain indices where edge_mask has at least one failure (1)\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in self.indices:\n",
    "            e_start = self.meta_dict['edge_index'][idx].item()\n",
    "            e_end = self.meta_dict['edge_index'][idx+1].item()\n",
    "            edge_mask = self.data_flat.edge_mask[e_start:e_end]  # Use edge_mask\n",
    "            if edge_mask.sum() > 0:  # At least one failed edge\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subgraph_id = self.indices[idx]\n",
    "        return get_subgraph(self.data_flat, self.meta_dict, subgraph_id)\n",
    "\n",
    "# Create dataset (only Category A graphs)\n",
    "full_dataset = PowerGraphDataset(loaded_data[0], loaded_data[1], filter_category_A=True)\n",
    "print(\"Total subgraphs in full_dataset:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge label distribution:\n",
      "- Failed edges (1): 2360.0 (0.55%)\n",
      "- Stable edges (0): 428464.0 (99.45%)\n"
     ]
    }
   ],
   "source": [
    "# After creating full_dataset (Cell 5):\n",
    "all_edge_masks = torch.cat([batch.edge_mask for batch in full_dataset])\n",
    "num_positive = all_edge_masks.sum().item()\n",
    "num_negative = len(all_edge_masks) - num_positive\n",
    "\n",
    "print(f\"Edge label distribution:\")\n",
    "print(f\"- Failed edges (1): {num_positive} ({num_positive / len(all_edge_masks):.2%})\")\n",
    "print(f\"- Stable edges (0): {num_negative} ({num_negative / len(all_edge_masks):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 933\n",
      "Val   set size: 116\n",
      "Test  set size: 118\n",
      "DataLoaders created with batch_size = 128\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train/Val/Test split & DataLoaders (with class-aware sampling)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# 1) Handle extreme class imbalance\n",
    "# --------------------------------------------------------\n",
    "num_subgraphs = len(full_dataset)\n",
    "train_size   = int(0.8 * num_subgraphs)\n",
    "val_size     = int(0.1 * num_subgraphs)\n",
    "test_size    = num_subgraphs - train_size - val_size\n",
    "\n",
    "# 2) Random split (with fixed seed for reproducibility)\n",
    "indices = np.arange(num_subgraphs)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx   = indices[train_size:train_size+val_size]\n",
    "test_idx  = indices[train_size+val_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset   = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset  = torch.utils.data.Subset(full_dataset, test_idx)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Val   set size: {len(val_dataset)}\")\n",
    "print(f\"Test  set size: {len(test_dataset)}\")\n",
    "\n",
    "# 3) Build PyG DataLoaders with class-aware sampling\n",
    "batch_size = 128\n",
    "\n",
    "# Compute per-graph positive‐edge fraction\n",
    "graph_pos_frac = [g.edge_mask.float().mean().item() for g in full_dataset]\n",
    "# Inverse weighting: draw more from graphs with fewer positives\n",
    "graph_weights  = [1.0 / (p if p > 0 else 1e-4) for p in graph_pos_frac]\n",
    "train_weights  = [graph_weights[i] for i in train_idx]\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=train_weights,\n",
    "    num_samples=len(train_idx),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = PyGDataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with batch_size =\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight = 13.5  (used by BCEWithLogitsLoss)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6‑bis – compute pos_weight once (run AFTER Cell 6)\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ─────────────────────────────────────────────────────────\n",
    "#   Needed globally for class‑balanced BCE loss\n",
    "# ─────────────────────────────────────────────────────────\n",
    "all_edge_masks = torch.cat([g.edge_mask for g in full_dataset])\n",
    "num_pos = all_edge_masks.sum().item()\n",
    "num_neg = len(all_edge_masks) - num_pos\n",
    "pos_weight = torch.tensor([(num_neg / num_pos) ** 0.5], device=device) \n",
    "\n",
    "print(f\"pos_weight = {pos_weight.item():.1f}  (used by BCEWithLogitsLoss)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Architectures (aligned to enhanced pipeline)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log\n",
    "from torch_geometric.nn import GINEConv, GATConv, NNConv\n",
    "\n",
    "# ───── Helpers ────────────────────────────────────\n",
    "# assumes you’ve already computed pos_weight (in Cell 6‑bis) and have it on the correct device\n",
    "BIAS_INIT = -log(pos_weight.item())\n",
    "\n",
    "def make_input_norm(n_feats):\n",
    "    return nn.LayerNorm(n_feats, elementwise_affine=True)\n",
    "\n",
    "# ───── GINE ───────────────────────────────────────\n",
    "class GINEBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # per‑node & per‑edge LayerNorm\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        # initial projection\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        # GINE conv needs an MLP\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        self.conv1 = GINEConv(nn=mlp, edge_dim=in_channels_edge)\n",
    "        self.conv2 = GINEConv(nn=mlp, edge_dim=in_channels_edge)\n",
    "\n",
    "        # edge‑prediction MLP\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # bias init for class imbalance\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index, edge_attr))\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n",
    "\n",
    "# ───── GAT ────────────────────────────────────────\n",
    "class GATBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        # 2 heads of size hidden_dim//4 each, concatenated back to hidden_dim\n",
    "        self.conv1 = GATConv(hidden_dim, hidden_dim // 4, heads=4, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim // 4, heads=4, concat=True)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index))\n",
    "        h = self.conv2(h, edge_index)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n",
    "\n",
    "# ───── Edge‑aware NNConv ───────────────────────────\n",
    "class EdgeAwareGraphConvClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "\n",
    "        # build the edge network for NNConv\n",
    "        edge_net = nn.Sequential(\n",
    "            nn.Linear(in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        self.conv1 = NNConv(hidden_dim, hidden_dim, edge_net)\n",
    "        self.conv2 = NNConv(hidden_dim, hidden_dim, edge_net)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index, edge_attr))\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────\n",
    "# 📍 Cell 8 – Training + Evaluation utilities\n",
    "# ────────────────────────────────────────────\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ---- metrics ----------------------------------------------------\n",
    "def calculate_metrics(preds, labels):\n",
    "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    rec  = tp / (tp + fn + 1e-8)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    return prec, rec, f1\n",
    "\n",
    "# ---- evaluation -------------------------------------------------\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, all_logits, all_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss   = criterion(logits, batch.edge_mask.float())\n",
    "            total_loss += loss.item()\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(batch.edge_mask.float())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    probs = torch.sigmoid(all_logits)\n",
    "\n",
    "    # find best threshold on validation\n",
    "    best_f1, best_thr = 0, 0\n",
    "    for thr in np.logspace(-6, -1, 30):\n",
    "        f1 = calculate_metrics((probs > thr).long(), all_labels.long())[2]\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "\n",
    "    prec, rec, f1 = calculate_metrics((probs > best_thr).long(), all_labels.long())\n",
    "    return total_loss / len(loader), {'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "# ---- training + full history -----------------------------------\n",
    "def train_and_evaluate_model(model, model_name,\n",
    "                             train_loader, val_loader,\n",
    "                             device,\n",
    "                             num_epochs=30,\n",
    "                             patience=7,\n",
    "                             lr=5e-4):\n",
    "    \"\"\"\n",
    "    Train `model`, evaluate on `val_loader` each epoch, \n",
    "    and return a history dict with lists for train_loss, val_loss, precision, recall, f1.\n",
    "    \"\"\"\n",
    "    # you can swap in BCEWithLogitsLoss(pos_weight=...) if you like\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss':   [],\n",
    "        'precision':  [],\n",
    "        'recall':     [],\n",
    "        'f1':         []\n",
    "    }\n",
    "\n",
    "    best_f1, wait = 0.0, 0\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # --- training pass ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            # in‑batch sampler for imbalance\n",
    "            pos = batch.edge_mask.nonzero(as_tuple=True)[0]\n",
    "            neg = (batch.edge_mask == 0).nonzero(as_tuple=True)[0]\n",
    "            k   = min(len(neg), 5 * len(pos))\n",
    "            if len(pos)>0 and k>0:\n",
    "                neg_idx = neg[torch.randperm(len(neg), device=neg.device)[:k]]\n",
    "                sel = torch.cat([pos, neg_idx])\n",
    "            else:\n",
    "                sel = torch.arange(len(batch.edge_mask), device=logits.device)\n",
    "\n",
    "            loss = criterion(logits[sel], batch.edge_mask.float()[sel])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # --- validation pass ---\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, device, criterion)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['precision'].append(val_metrics['precision'])\n",
    "        history['recall'].append(val_metrics['recall'])\n",
    "        history['f1'].append(val_metrics['f1'])\n",
    "\n",
    "        print(f\"{model_name} | Ep {epoch:02d}  Train {avg_train_loss:.4f}  \"\n",
    "              f\"ValF1 {val_metrics['f1']:.4f}\")\n",
    "\n",
    "        # early stopping\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1, wait = val_metrics['f1'], 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"⏹️ early stop at {epoch}, best F1={best_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "[GINE] Fold 1/2\n",
      "GINE-F1 | Ep 01  Train 0.4746  ValF1 0.0109\n",
      "GINE-F1 | Ep 02  Train 0.4164  ValF1 0.0128\n",
      "GINE-F1 | Ep 03  Train 0.3743  ValF1 0.0215\n",
      "GINE-F1 | Ep 04  Train 0.3457  ValF1 0.0203\n",
      "GINE-F1 | Ep 05  Train 0.3025  ValF1 0.0306\n",
      "GINE-F1 | Ep 06  Train 0.2610  ValF1 0.0342\n",
      "GINE-F1 | Ep 07  Train 0.2380  ValF1 0.0641\n",
      "GINE-F1 | Ep 08  Train 0.2073  ValF1 0.0688\n",
      "GINE-F1 | Ep 09  Train 0.1910  ValF1 0.1197\n",
      "GINE-F1 | Ep 10  Train 0.1712  ValF1 0.1506\n",
      "GINE-F1 | Ep 11  Train 0.1601  ValF1 0.1777\n",
      "GINE-F1 | Ep 12  Train 0.1411  ValF1 0.1804\n",
      "GINE-F1 | Ep 13  Train 0.1262  ValF1 0.1805\n",
      "GINE-F1 | Ep 14  Train 0.1210  ValF1 0.2306\n",
      "GINE-F1 | Ep 15  Train 0.1123  ValF1 0.2105\n",
      "GINE-F1 | Ep 16  Train 0.1054  ValF1 0.2554\n",
      "GINE-F1 | Ep 17  Train 0.0888  ValF1 0.2832\n",
      "GINE-F1 | Ep 18  Train 0.0950  ValF1 0.2529\n",
      "GINE-F1 | Ep 19  Train 0.0916  ValF1 0.2832\n",
      "GINE-F1 | Ep 20  Train 0.0780  ValF1 0.2605\n",
      "GINE-F1 | Ep 21  Train 0.0905  ValF1 0.3228\n",
      "GINE-F1 | Ep 22  Train 0.0764  ValF1 0.3021\n",
      "GINE-F1 | Ep 23  Train 0.0741  ValF1 0.3240\n",
      "GINE-F1 | Ep 24  Train 0.0673  ValF1 0.3953\n",
      "GINE-F1 | Ep 25  Train 0.0643  ValF1 0.3349\n",
      "GINE-F1 | Ep 26  Train 0.0531  ValF1 0.4248\n",
      "GINE-F1 | Ep 27  Train 0.0657  ValF1 0.3789\n",
      "GINE-F1 | Ep 28  Train 0.0596  ValF1 0.3814\n",
      "GINE-F1 | Ep 29  Train 0.0590  ValF1 0.4139\n",
      "GINE-F1 | Ep 30  Train 0.0439  ValF1 0.4138\n",
      "GINE-F1 | Ep 31  Train 0.0517  ValF1 0.4476\n",
      "GINE-F1 | Ep 32  Train 0.0519  ValF1 0.3939\n",
      "GINE-F1 | Ep 33  Train 0.0460  ValF1 0.4853\n",
      "GINE-F1 | Ep 34  Train 0.0477  ValF1 0.4315\n",
      "GINE-F1 | Ep 35  Train 0.0458  ValF1 0.4719\n",
      "GINE-F1 | Ep 36  Train 0.0497  ValF1 0.5250\n",
      "GINE-F1 | Ep 37  Train 0.0447  ValF1 0.4360\n",
      "GINE-F1 | Ep 38  Train 0.0507  ValF1 0.4771\n",
      "GINE-F1 | Ep 39  Train 0.0507  ValF1 0.6112\n",
      "GINE-F1 | Ep 40  Train 0.0427  ValF1 0.4673\n",
      "GINE-F1 | Ep 41  Train 0.0391  ValF1 0.5133\n",
      "GINE-F1 | Ep 42  Train 0.0479  ValF1 0.5530\n",
      "GINE-F1 | Ep 43  Train 0.0420  ValF1 0.5523\n",
      "GINE-F1 | Ep 44  Train 0.0348  ValF1 0.5351\n",
      "GINE-F1 | Ep 45  Train 0.0380  ValF1 0.5832\n",
      "GINE-F1 | Ep 46  Train 0.0358  ValF1 0.5411\n",
      "⏹️ early stop at 46, best F1=0.6112\n",
      "\n",
      "[GINE] Fold 2/2\n",
      "GINE-F2 | Ep 01  Train 0.4577  ValF1 0.0116\n",
      "GINE-F2 | Ep 02  Train 0.3691  ValF1 0.0266\n",
      "GINE-F2 | Ep 03  Train 0.3101  ValF1 0.0305\n",
      "GINE-F2 | Ep 04  Train 0.2534  ValF1 0.0626\n",
      "GINE-F2 | Ep 05  Train 0.2064  ValF1 0.0616\n",
      "GINE-F2 | Ep 06  Train 0.1868  ValF1 0.1240\n",
      "GINE-F2 | Ep 07  Train 0.1451  ValF1 0.1583\n",
      "GINE-F2 | Ep 08  Train 0.1335  ValF1 0.1832\n",
      "GINE-F2 | Ep 09  Train 0.1230  ValF1 0.2092\n",
      "GINE-F2 | Ep 10  Train 0.1087  ValF1 0.2332\n",
      "GINE-F2 | Ep 11  Train 0.1141  ValF1 0.2431\n",
      "GINE-F2 | Ep 12  Train 0.0934  ValF1 0.2813\n",
      "GINE-F2 | Ep 13  Train 0.0984  ValF1 0.2796\n",
      "GINE-F2 | Ep 14  Train 0.0950  ValF1 0.3067\n",
      "GINE-F2 | Ep 15  Train 0.0821  ValF1 0.2961\n",
      "GINE-F2 | Ep 16  Train 0.0771  ValF1 0.3492\n",
      "GINE-F2 | Ep 17  Train 0.0809  ValF1 0.2607\n",
      "GINE-F2 | Ep 18  Train 0.0800  ValF1 0.4390\n",
      "GINE-F2 | Ep 19  Train 0.0729  ValF1 0.1855\n",
      "GINE-F2 | Ep 20  Train 0.0733  ValF1 0.4689\n",
      "GINE-F2 | Ep 21  Train 0.0641  ValF1 0.2838\n",
      "GINE-F2 | Ep 22  Train 0.0632  ValF1 0.4068\n",
      "GINE-F2 | Ep 23  Train 0.0624  ValF1 0.3817\n",
      "GINE-F2 | Ep 24  Train 0.0584  ValF1 0.4816\n",
      "GINE-F2 | Ep 25  Train 0.0472  ValF1 0.3914\n",
      "GINE-F2 | Ep 26  Train 0.0658  ValF1 0.4761\n",
      "GINE-F2 | Ep 27  Train 0.0459  ValF1 0.4838\n",
      "GINE-F2 | Ep 28  Train 0.0524  ValF1 0.3747\n",
      "GINE-F2 | Ep 29  Train 0.0573  ValF1 0.5246\n",
      "GINE-F2 | Ep 30  Train 0.0652  ValF1 0.3665\n",
      "GINE-F2 | Ep 31  Train 0.0528  ValF1 0.5290\n",
      "GINE-F2 | Ep 32  Train 0.0422  ValF1 0.5162\n",
      "GINE-F2 | Ep 33  Train 0.0573  ValF1 0.4351\n",
      "GINE-F2 | Ep 34  Train 0.0432  ValF1 0.5627\n",
      "GINE-F2 | Ep 35  Train 0.0430  ValF1 0.4829\n",
      "GINE-F2 | Ep 36  Train 0.0557  ValF1 0.4754\n",
      "GINE-F2 | Ep 37  Train 0.0552  ValF1 0.5101\n",
      "GINE-F2 | Ep 38  Train 0.0414  ValF1 0.4635\n",
      "GINE-F2 | Ep 39  Train 0.0426  ValF1 0.5655\n",
      "GINE-F2 | Ep 40  Train 0.0374  ValF1 0.5186\n",
      "GINE-F2 | Ep 41  Train 0.0381  ValF1 0.5770\n",
      "GINE-F2 | Ep 42  Train 0.0407  ValF1 0.5238\n",
      "GINE-F2 | Ep 43  Train 0.0391  ValF1 0.5713\n",
      "GINE-F2 | Ep 44  Train 0.0314  ValF1 0.5747\n",
      "GINE-F2 | Ep 45  Train 0.0369  ValF1 0.5725\n",
      "GINE-F2 | Ep 46  Train 0.0351  ValF1 0.5991\n",
      "GINE-F2 | Ep 47  Train 0.0454  ValF1 0.5962\n",
      "GINE-F2 | Ep 48  Train 0.0389  ValF1 0.6013\n",
      "GINE-F2 | Ep 49  Train 0.0383  ValF1 0.5598\n",
      "GINE-F2 | Ep 50  Train 0.0280  ValF1 0.6289\n",
      "GINE-F2 | Ep 51  Train 0.0320  ValF1 0.5012\n",
      "GINE-F2 | Ep 52  Train 0.0370  ValF1 0.6477\n",
      "GINE-F2 | Ep 53  Train 0.0385  ValF1 0.5940\n",
      "GINE-F2 | Ep 54  Train 0.0319  ValF1 0.5716\n",
      "GINE-F2 | Ep 55  Train 0.0444  ValF1 0.6346\n",
      "GINE-F2 | Ep 56  Train 0.0324  ValF1 0.5700\n",
      "GINE-F2 | Ep 57  Train 0.0357  ValF1 0.6149\n",
      "GINE-F2 | Ep 58  Train 0.0345  ValF1 0.6992\n",
      "GINE-F2 | Ep 59  Train 0.0419  ValF1 0.6430\n",
      "GINE-F2 | Ep 60  Train 0.0339  ValF1 0.5326\n",
      "GINE-F2 | Ep 61  Train 0.0340  ValF1 0.7457\n",
      "GINE-F2 | Ep 62  Train 0.0391  ValF1 0.4355\n",
      "GINE-F2 | Ep 63  Train 0.0422  ValF1 0.7588\n",
      "GINE-F2 | Ep 64  Train 0.0290  ValF1 0.5337\n",
      "GINE-F2 | Ep 65  Train 0.0449  ValF1 0.6867\n",
      "GINE-F2 | Ep 66  Train 0.0444  ValF1 0.3999\n",
      "GINE-F2 | Ep 67  Train 0.0313  ValF1 0.7615\n",
      "GINE-F2 | Ep 68  Train 0.0486  ValF1 0.6005\n",
      "GINE-F2 | Ep 69  Train 0.0336  ValF1 0.6804\n",
      "GINE-F2 | Ep 70  Train 0.0398  ValF1 0.5921\n",
      "GINE-F2 | Ep 71  Train 0.0403  ValF1 0.7289\n",
      "GINE-F2 | Ep 72  Train 0.0403  ValF1 0.5704\n",
      "GINE-F2 | Ep 73  Train 0.0366  ValF1 0.6796\n",
      "GINE-F2 | Ep 74  Train 0.0354  ValF1 0.6031\n",
      "⏹️ early stop at 74, best F1=0.7615\n",
      "\n",
      "[GAT] Fold 1/2\n",
      "GAT-F1 | Ep 01  Train 0.6037  ValF1 0.0152\n",
      "GAT-F1 | Ep 02  Train 0.4485  ValF1 0.0110\n",
      "GAT-F1 | Ep 03  Train 0.4140  ValF1 0.0115\n",
      "GAT-F1 | Ep 04  Train 0.3748  ValF1 0.0205\n",
      "GAT-F1 | Ep 05  Train 0.3547  ValF1 0.0275\n",
      "GAT-F1 | Ep 06  Train 0.3080  ValF1 0.0245\n",
      "GAT-F1 | Ep 07  Train 0.2705  ValF1 0.0324\n",
      "GAT-F1 | Ep 08  Train 0.2364  ValF1 0.0553\n",
      "GAT-F1 | Ep 09  Train 0.2076  ValF1 0.0689\n",
      "GAT-F1 | Ep 10  Train 0.1998  ValF1 0.0780\n",
      "GAT-F1 | Ep 11  Train 0.1761  ValF1 0.1045\n",
      "GAT-F1 | Ep 12  Train 0.1665  ValF1 0.1304\n",
      "GAT-F1 | Ep 13  Train 0.1394  ValF1 0.1520\n",
      "GAT-F1 | Ep 14  Train 0.1273  ValF1 0.2204\n",
      "GAT-F1 | Ep 15  Train 0.1137  ValF1 0.2130\n",
      "GAT-F1 | Ep 16  Train 0.1157  ValF1 0.2429\n",
      "GAT-F1 | Ep 17  Train 0.1079  ValF1 0.3062\n",
      "GAT-F1 | Ep 18  Train 0.0885  ValF1 0.3350\n",
      "GAT-F1 | Ep 19  Train 0.0974  ValF1 0.3834\n",
      "GAT-F1 | Ep 20  Train 0.0898  ValF1 0.3603\n",
      "GAT-F1 | Ep 21  Train 0.0941  ValF1 0.2823\n",
      "GAT-F1 | Ep 22  Train 0.0876  ValF1 0.3628\n",
      "GAT-F1 | Ep 23  Train 0.0766  ValF1 0.3561\n",
      "GAT-F1 | Ep 24  Train 0.0728  ValF1 0.4763\n",
      "GAT-F1 | Ep 25  Train 0.0689  ValF1 0.4396\n",
      "GAT-F1 | Ep 26  Train 0.0712  ValF1 0.4610\n",
      "GAT-F1 | Ep 27  Train 0.0754  ValF1 0.4532\n",
      "GAT-F1 | Ep 28  Train 0.0558  ValF1 0.5012\n",
      "GAT-F1 | Ep 29  Train 0.0596  ValF1 0.5415\n",
      "GAT-F1 | Ep 30  Train 0.0640  ValF1 0.4846\n",
      "GAT-F1 | Ep 31  Train 0.0565  ValF1 0.4897\n",
      "GAT-F1 | Ep 32  Train 0.0537  ValF1 0.5305\n",
      "GAT-F1 | Ep 33  Train 0.0658  ValF1 0.4812\n",
      "GAT-F1 | Ep 34  Train 0.0501  ValF1 0.5207\n",
      "GAT-F1 | Ep 35  Train 0.0502  ValF1 0.5263\n",
      "GAT-F1 | Ep 36  Train 0.0449  ValF1 0.5558\n",
      "GAT-F1 | Ep 37  Train 0.0589  ValF1 0.5028\n",
      "GAT-F1 | Ep 38  Train 0.0549  ValF1 0.5218\n",
      "GAT-F1 | Ep 39  Train 0.0584  ValF1 0.5076\n",
      "GAT-F1 | Ep 40  Train 0.0519  ValF1 0.5005\n",
      "GAT-F1 | Ep 41  Train 0.0455  ValF1 0.5375\n",
      "GAT-F1 | Ep 42  Train 0.0475  ValF1 0.5442\n",
      "GAT-F1 | Ep 43  Train 0.0486  ValF1 0.4486\n",
      "⏹️ early stop at 43, best F1=0.5558\n",
      "\n",
      "[GAT] Fold 2/2\n",
      "GAT-F2 | Ep 01  Train 0.5806  ValF1 0.0282\n",
      "GAT-F2 | Ep 02  Train 0.4304  ValF1 0.0150\n",
      "GAT-F2 | Ep 03  Train 0.3744  ValF1 0.0181\n",
      "GAT-F2 | Ep 04  Train 0.3256  ValF1 0.0378\n",
      "GAT-F2 | Ep 05  Train 0.3138  ValF1 0.0492\n",
      "GAT-F2 | Ep 06  Train 0.2850  ValF1 0.0455\n",
      "GAT-F2 | Ep 07  Train 0.2614  ValF1 0.0542\n",
      "GAT-F2 | Ep 08  Train 0.2334  ValF1 0.0914\n",
      "GAT-F2 | Ep 09  Train 0.2101  ValF1 0.1132\n",
      "GAT-F2 | Ep 10  Train 0.1960  ValF1 0.1180\n",
      "GAT-F2 | Ep 11  Train 0.1787  ValF1 0.1501\n",
      "GAT-F2 | Ep 12  Train 0.1696  ValF1 0.2137\n",
      "GAT-F2 | Ep 13  Train 0.1611  ValF1 0.2202\n",
      "GAT-F2 | Ep 14  Train 0.1406  ValF1 0.2842\n",
      "GAT-F2 | Ep 15  Train 0.1417  ValF1 0.2897\n",
      "GAT-F2 | Ep 16  Train 0.1285  ValF1 0.3243\n",
      "GAT-F2 | Ep 17  Train 0.1245  ValF1 0.3701\n",
      "GAT-F2 | Ep 18  Train 0.1254  ValF1 0.3785\n",
      "GAT-F2 | Ep 19  Train 0.1063  ValF1 0.5046\n",
      "GAT-F2 | Ep 20  Train 0.1149  ValF1 0.5134\n",
      "GAT-F2 | Ep 21  Train 0.1078  ValF1 0.5092\n",
      "GAT-F2 | Ep 22  Train 0.0913  ValF1 0.5718\n",
      "GAT-F2 | Ep 23  Train 0.0964  ValF1 0.5674\n",
      "GAT-F2 | Ep 24  Train 0.0969  ValF1 0.5733\n",
      "GAT-F2 | Ep 25  Train 0.0833  ValF1 0.5786\n",
      "GAT-F2 | Ep 26  Train 0.1059  ValF1 0.5787\n",
      "GAT-F2 | Ep 27  Train 0.0839  ValF1 0.5878\n",
      "GAT-F2 | Ep 28  Train 0.0752  ValF1 0.5890\n",
      "GAT-F2 | Ep 29  Train 0.0881  ValF1 0.5805\n",
      "GAT-F2 | Ep 30  Train 0.0680  ValF1 0.5826\n",
      "GAT-F2 | Ep 31  Train 0.0687  ValF1 0.5886\n",
      "GAT-F2 | Ep 32  Train 0.0828  ValF1 0.5929\n",
      "GAT-F2 | Ep 33  Train 0.0717  ValF1 0.5772\n",
      "GAT-F2 | Ep 34  Train 0.0831  ValF1 0.5639\n",
      "GAT-F2 | Ep 35  Train 0.0650  ValF1 0.6002\n",
      "GAT-F2 | Ep 36  Train 0.0800  ValF1 0.5139\n",
      "GAT-F2 | Ep 37  Train 0.0786  ValF1 0.4204\n",
      "GAT-F2 | Ep 38  Train 0.0757  ValF1 0.4421\n",
      "GAT-F2 | Ep 39  Train 0.0755  ValF1 0.4740\n",
      "GAT-F2 | Ep 40  Train 0.0706  ValF1 0.4428\n",
      "GAT-F2 | Ep 41  Train 0.0794  ValF1 0.4452\n",
      "GAT-F2 | Ep 42  Train 0.0659  ValF1 0.5377\n",
      "⏹️ early stop at 42, best F1=0.6002\n",
      "\n",
      "[EdgeAwareGC] Fold 1/2\n",
      "EdgeAwareGC-F1 | Ep 01  Train 0.4210  ValF1 0.0252\n",
      "EdgeAwareGC-F1 | Ep 02  Train 0.3011  ValF1 0.0558\n",
      "EdgeAwareGC-F1 | Ep 03  Train 0.2372  ValF1 0.1027\n",
      "EdgeAwareGC-F1 | Ep 04  Train 0.1964  ValF1 0.1301\n",
      "EdgeAwareGC-F1 | Ep 05  Train 0.1658  ValF1 0.1497\n",
      "EdgeAwareGC-F1 | Ep 06  Train 0.1590  ValF1 0.1745\n",
      "EdgeAwareGC-F1 | Ep 07  Train 0.1278  ValF1 0.1932\n",
      "EdgeAwareGC-F1 | Ep 08  Train 0.1387  ValF1 0.2226\n",
      "EdgeAwareGC-F1 | Ep 09  Train 0.1140  ValF1 0.2365\n",
      "EdgeAwareGC-F1 | Ep 10  Train 0.1036  ValF1 0.3116\n",
      "EdgeAwareGC-F1 | Ep 11  Train 0.1014  ValF1 0.3134\n",
      "EdgeAwareGC-F1 | Ep 12  Train 0.0886  ValF1 0.3973\n",
      "EdgeAwareGC-F1 | Ep 13  Train 0.0857  ValF1 0.3995\n",
      "EdgeAwareGC-F1 | Ep 14  Train 0.0811  ValF1 0.4416\n",
      "EdgeAwareGC-F1 | Ep 15  Train 0.0801  ValF1 0.4892\n",
      "EdgeAwareGC-F1 | Ep 16  Train 0.0688  ValF1 0.5499\n",
      "EdgeAwareGC-F1 | Ep 17  Train 0.0747  ValF1 0.5428\n",
      "EdgeAwareGC-F1 | Ep 18  Train 0.0633  ValF1 0.5276\n",
      "EdgeAwareGC-F1 | Ep 19  Train 0.0703  ValF1 0.5077\n",
      "EdgeAwareGC-F1 | Ep 20  Train 0.0572  ValF1 0.5942\n",
      "EdgeAwareGC-F1 | Ep 21  Train 0.0566  ValF1 0.5047\n",
      "EdgeAwareGC-F1 | Ep 22  Train 0.0550  ValF1 0.6395\n",
      "EdgeAwareGC-F1 | Ep 23  Train 0.0494  ValF1 0.4942\n",
      "EdgeAwareGC-F1 | Ep 24  Train 0.0421  ValF1 0.6449\n",
      "EdgeAwareGC-F1 | Ep 25  Train 0.0492  ValF1 0.6013\n",
      "EdgeAwareGC-F1 | Ep 26  Train 0.0510  ValF1 0.4700\n",
      "EdgeAwareGC-F1 | Ep 27  Train 0.0432  ValF1 0.5900\n",
      "EdgeAwareGC-F1 | Ep 28  Train 0.0423  ValF1 0.5516\n",
      "EdgeAwareGC-F1 | Ep 29  Train 0.0445  ValF1 0.6136\n",
      "EdgeAwareGC-F1 | Ep 30  Train 0.0453  ValF1 0.5528\n",
      "EdgeAwareGC-F1 | Ep 31  Train 0.0513  ValF1 0.6745\n",
      "EdgeAwareGC-F1 | Ep 32  Train 0.0358  ValF1 0.6411\n",
      "EdgeAwareGC-F1 | Ep 33  Train 0.0283  ValF1 0.6851\n",
      "EdgeAwareGC-F1 | Ep 34  Train 0.0308  ValF1 0.6094\n",
      "EdgeAwareGC-F1 | Ep 35  Train 0.0324  ValF1 0.6681\n",
      "EdgeAwareGC-F1 | Ep 36  Train 0.0411  ValF1 0.6800\n",
      "EdgeAwareGC-F1 | Ep 37  Train 0.0253  ValF1 0.6820\n",
      "EdgeAwareGC-F1 | Ep 38  Train 0.0349  ValF1 0.7175\n",
      "EdgeAwareGC-F1 | Ep 39  Train 0.0258  ValF1 0.6504\n",
      "EdgeAwareGC-F1 | Ep 40  Train 0.0332  ValF1 0.6738\n",
      "EdgeAwareGC-F1 | Ep 41  Train 0.0426  ValF1 0.6975\n",
      "EdgeAwareGC-F1 | Ep 42  Train 0.0275  ValF1 0.7059\n",
      "EdgeAwareGC-F1 | Ep 43  Train 0.0304  ValF1 0.6652\n",
      "EdgeAwareGC-F1 | Ep 44  Train 0.0262  ValF1 0.7111\n",
      "EdgeAwareGC-F1 | Ep 45  Train 0.0292  ValF1 0.6988\n",
      "⏹️ early stop at 45, best F1=0.7175\n",
      "\n",
      "[EdgeAwareGC] Fold 2/2\n",
      "EdgeAwareGC-F2 | Ep 01  Train 0.4594  ValF1 0.0226\n",
      "EdgeAwareGC-F2 | Ep 02  Train 0.2646  ValF1 0.0584\n",
      "EdgeAwareGC-F2 | Ep 03  Train 0.2064  ValF1 0.1050\n",
      "EdgeAwareGC-F2 | Ep 04  Train 0.1674  ValF1 0.1519\n",
      "EdgeAwareGC-F2 | Ep 05  Train 0.1531  ValF1 0.2065\n",
      "EdgeAwareGC-F2 | Ep 06  Train 0.1446  ValF1 0.1883\n",
      "EdgeAwareGC-F2 | Ep 07  Train 0.1291  ValF1 0.2745\n",
      "EdgeAwareGC-F2 | Ep 08  Train 0.1107  ValF1 0.2747\n",
      "EdgeAwareGC-F2 | Ep 09  Train 0.1090  ValF1 0.3470\n",
      "EdgeAwareGC-F2 | Ep 10  Train 0.1022  ValF1 0.3546\n",
      "EdgeAwareGC-F2 | Ep 11  Train 0.0907  ValF1 0.3928\n",
      "EdgeAwareGC-F2 | Ep 12  Train 0.0921  ValF1 0.4665\n",
      "EdgeAwareGC-F2 | Ep 13  Train 0.0831  ValF1 0.4479\n",
      "EdgeAwareGC-F2 | Ep 14  Train 0.0764  ValF1 0.4766\n",
      "EdgeAwareGC-F2 | Ep 15  Train 0.0814  ValF1 0.5401\n",
      "EdgeAwareGC-F2 | Ep 16  Train 0.0772  ValF1 0.5748\n",
      "EdgeAwareGC-F2 | Ep 17  Train 0.0655  ValF1 0.5506\n",
      "EdgeAwareGC-F2 | Ep 18  Train 0.0572  ValF1 0.6001\n",
      "EdgeAwareGC-F2 | Ep 19  Train 0.0680  ValF1 0.4764\n",
      "EdgeAwareGC-F2 | Ep 20  Train 0.0636  ValF1 0.6701\n",
      "EdgeAwareGC-F2 | Ep 21  Train 0.0477  ValF1 0.5357\n",
      "EdgeAwareGC-F2 | Ep 22  Train 0.0636  ValF1 0.5368\n",
      "EdgeAwareGC-F2 | Ep 23  Train 0.0440  ValF1 0.6927\n",
      "EdgeAwareGC-F2 | Ep 24  Train 0.0572  ValF1 0.5848\n",
      "EdgeAwareGC-F2 | Ep 25  Train 0.0417  ValF1 0.6304\n",
      "EdgeAwareGC-F2 | Ep 26  Train 0.0578  ValF1 0.6667\n",
      "EdgeAwareGC-F2 | Ep 27  Train 0.0477  ValF1 0.6126\n",
      "EdgeAwareGC-F2 | Ep 28  Train 0.0485  ValF1 0.6475\n",
      "EdgeAwareGC-F2 | Ep 29  Train 0.0351  ValF1 0.6285\n",
      "EdgeAwareGC-F2 | Ep 30  Train 0.0277  ValF1 0.6244\n",
      "⏹️ early stop at 30, best F1=0.6927\n",
      "\n",
      "📊  Fast 2‑Fold Summary (mean ± std)\n",
      "GINE:  F1 0.5721±0.0310, Prec 0.4047±0.0298, Rec 0.9788±0.0063\n",
      "GAT:  F1 0.4932±0.0446, Prec 0.3317±0.0402, Rec 0.9713±0.0011\n",
      "EdgeAwareGC:  F1 0.6616±0.0372, Prec 0.5014±0.0433, Rec 0.9777±0.0031\n"
     ]
    }
   ],
   "source": [
    "# 📍 Cell 9 – Fast 2‑fold run across models with full metrics saving\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def run_kfold_full(model_name, model_class, dataset, indices,\n",
    "                   K=2, num_epochs=100, patience=7):\n",
    "    \"\"\"\n",
    "    Runs K‑fold CV, returns:\n",
    "      - fold_histories: list of per-fold history dicts\n",
    "      - summary: dict of mean±std for precision, recall, f1 across folds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "    fold_histories = []\n",
    "    all_prec, all_rec, all_f1 = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "        print(f\"\\n[{model_name}] Fold {fold}/{K}\")\n",
    "        # build subsets\n",
    "        tr_ids = [indices[i] for i in train_idx]\n",
    "        vl_ids = [indices[i] for i in val_idx]\n",
    "        tr_ds = torch.utils.data.Subset(dataset, tr_ids)\n",
    "        vl_ds = torch.utils.data.Subset(dataset, vl_ids)\n",
    "\n",
    "        # weighted sampler to handle imbalance\n",
    "        frac_pos = [g.edge_mask.float().mean().item() for g in dataset]\n",
    "        weights = [1.0/(p if p>0 else 1e-4) for p in frac_pos]\n",
    "        sampler = WeightedRandomSampler([weights[i] for i in tr_ids],\n",
    "                                        len(tr_ids), replacement=True)\n",
    "\n",
    "        tr_loader = PyGDataLoader(tr_ds, batch_size=128, sampler=sampler)\n",
    "        vl_loader = PyGDataLoader(vl_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "        # init model\n",
    "        model = model_class().to(device)\n",
    "        # train and get history\n",
    "        history = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            model_name=f\"{model_name}-F{fold}\",\n",
    "            train_loader=tr_loader,\n",
    "            val_loader=vl_loader,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs,\n",
    "            patience=patience\n",
    "        )\n",
    "        # capture final fold metrics (last epoch on validation)\n",
    "        final_prec = history['precision'][-1]\n",
    "        final_rec  = history['recall'][-1]\n",
    "        final_f1   = history['f1'][-1]\n",
    "\n",
    "        all_prec.append(final_prec)\n",
    "        all_rec.append(final_rec)\n",
    "        all_f1.append(final_f1)\n",
    "\n",
    "        # store everything\n",
    "        fold_histories.append({\n",
    "            'fold': fold,\n",
    "            'train_loss': history['train_loss'],\n",
    "            'val_loss':   history['val_loss'],\n",
    "            'precision':  history['precision'],\n",
    "            'recall':     history['recall'],\n",
    "            'f1':         history['f1']\n",
    "        })\n",
    "\n",
    "    # compute mean±std\n",
    "    summary = {\n",
    "        'precision': {'mean': float(np.mean(all_prec)), 'std': float(np.std(all_prec))},\n",
    "        'recall':    {'mean': float(np.mean(all_rec)),  'std': float(np.std(all_rec))},\n",
    "        'f1':        {'mean': float(np.mean(all_f1)),   'std': float(np.std(all_f1))}\n",
    "    }\n",
    "\n",
    "    return fold_histories, summary\n",
    "\n",
    "# ==== run all three models ====================================================\n",
    "model_classes = {\n",
    "    \"GINE\":        GINEBasedClassifier,\n",
    "    \"GAT\":         GATBasedClassifier,\n",
    "    \"EdgeAwareGC\": EdgeAwareGraphConvClassifier\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "for name, cls in model_classes.items():\n",
    "    histories, summary = run_kfold_full(\n",
    "        model_name=name,\n",
    "        model_class=cls,\n",
    "        dataset=full_dataset,\n",
    "        indices=train_idx,\n",
    "        K=2,\n",
    "        num_epochs=100,\n",
    "        patience=7\n",
    "    )\n",
    "    all_results[name] = {\n",
    "        'folds': histories,\n",
    "        'summary': summary\n",
    "    }\n",
    "\n",
    "# save full JSON\n",
    "with open(\"base_full_results_118_pbs.json\", \"w\") as fp:\n",
    "    json.dump(all_results, fp, indent=2)\n",
    "\n",
    "# print summary\n",
    "print(\"\\n📊  Fast 2‑Fold Summary (mean ± std)\")\n",
    "for m, metrics in all_results.items():\n",
    "    sp = metrics['summary']\n",
    "    print(f\"{m}:  F1 {sp['f1']['mean']:.4f}±{sp['f1']['std']:.4f}, \"\n",
    "          f\"Prec {sp['precision']['mean']:.4f}±{sp['precision']['std']:.4f}, \"\n",
    "          f\"Rec {sp['recall']['mean']:.4f}±{sp['recall']['std']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
