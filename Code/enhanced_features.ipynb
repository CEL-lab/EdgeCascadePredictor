{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['B_f_tot'])\n"
     ]
    }
   ],
   "source": [
    "#### Test\n",
    "\n",
    "import mat73\n",
    "\n",
    "# Load raw node feature data\n",
    "file_path = \"dataset/ieee24/ieee24/raw/Bf.mat\"\n",
    "data = mat73.loadmat(file_path)\n",
    "\n",
    "# Check available keys\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'tuple'>\n",
      "Length of dataset tuple: 2\n",
      "\n",
      "First element type: <class 'torch_geometric.data.data.Data'>\n",
      "\n",
      "Metadata keys: ['x', 'edge_index', 'edge_attr', 'y', 'edge_mask', 'idx']\n",
      "x: <class 'torch.Tensor'>, first 10: tensor([  0,  24,  48,  72,  96, 120, 144, 168, 192, 216])\n",
      "edge_index: <class 'torch.Tensor'>, first 10: tensor([  0,  74, 148, 222, 296, 370, 444, 518, 592, 666])\n",
      "edge_attr: <class 'torch.Tensor'>, first 10: tensor([  0,  74, 148, 222, 296, 370, 444, 518, 592, 666])\n",
      "y: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "edge_mask: <class 'torch.Tensor'>, first 10: tensor([  0,  74, 148, 222, 296, 370, 444, 518, 592, 666])\n",
      "idx: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.147100.bright04/ipykernel_3132783/1833354843.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "##### New Experiment Adaeze\n",
    "# Cell 1: Basic Imports and Data Loading\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GINEConv, GATConv, GraphConv\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'dataset/ieee24/ieee24/processed_r/data.pt'\n",
    "loaded_data = torch.load(file_path)\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Loaded data type:\", type(loaded_data))\n",
    "print(\"Length of dataset tuple:\", len(loaded_data))\n",
    "\n",
    "# Inspect first element (summary only)\n",
    "print(\"\\nFirst element type:\", type(loaded_data[0]))\n",
    "\n",
    "# Extract metadata dictionary\n",
    "metadata_dict = loaded_data[1]\n",
    "print(\"\\nMetadata keys:\", list(metadata_dict.keys()))\n",
    "\n",
    "# Preview metadata values (first 10 entries)\n",
    "for key, val in metadata_dict.items():\n",
    "    preview = val[:10] if hasattr(val, '__len__') else \"N/A\"\n",
    "    print(f\"{key}: {type(val)}, first 10: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph 0:\n",
      "Data(x=[24, 6], edge_index=[2, 74], edge_attr=[74, 7], y=[1, 1], edge_mask=[74])\n",
      "Edge mask values: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Enhanced get_subgraph with new features\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import degree, to_networkx\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def get_subgraph(data_flat, meta_dict, i):\n",
    "    \"\"\"\n",
    "    Enhanced function to reconstruct the i-th subgraph with additional features.\n",
    "\n",
    "    data_flat: The big flattened Data object (loaded_data[0])\n",
    "    meta_dict: The dictionary of offsets (loaded_data[1])\n",
    "    i        : Index of the subgraph we want to reconstruct\n",
    "\n",
    "    returns: a PyG Data object representing the i-th subgraph\n",
    "    \"\"\"\n",
    "    # 1) Node offsets\n",
    "    x_start = meta_dict['x'][i].item()\n",
    "    x_end   = meta_dict['x'][i+1].item()\n",
    "    x_i = data_flat.x[x_start:x_end]\n",
    "    \n",
    "    # 2) Edge offsets\n",
    "    e_start = meta_dict['edge_index'][i].item()\n",
    "    e_end   = meta_dict['edge_index'][i+1].item()\n",
    "    edge_index_i = data_flat.edge_index[:, e_start:e_end]\n",
    "    edge_attr_i  = data_flat.edge_attr[e_start:e_end]\n",
    "    \n",
    "    # 3) Load TRUE binary edge labels (explanation_mask)\n",
    "    edge_mask_i = data_flat.edge_mask[e_start:e_end].float()\n",
    "    \n",
    "    # Strict validation for edge_mask\n",
    "    if not torch.all(torch.isin(edge_mask_i, torch.tensor([0., 1.]))):\n",
    "        print(f\"BAD SUBGRAPH {i}:\")\n",
    "        print(\"Unique values:\", edge_mask_i.unique())\n",
    "        print(\"Edge indices:\", edge_index_i)\n",
    "        raise ValueError(\"Edge mask contains non-binary values\")\n",
    "    \n",
    "    # =====================\n",
    "    # New Node Features\n",
    "    # =====================\n",
    "    # Convert PyG edge index to NetworkX graph\n",
    "    G = to_networkx(Data(edge_index=edge_index_i, num_nodes=x_i.shape[0]), to_undirected=True)\n",
    "\n",
    "    # 1. Node Betweenness Centrality\n",
    "    node_betweenness_dict = nx.betweenness_centrality(G)\n",
    "    node_betweenness = torch.tensor([node_betweenness_dict.get(n, 0.0) for n in range(x_i.shape[0])], dtype=torch.float)\n",
    "\n",
    "    # 2. Node Degree (Fixed `.reshape(-1)`)\n",
    "    node_deg = degree(edge_index_i.reshape(-1), num_nodes=x_i.shape[0], dtype=torch.float)\n",
    "    \n",
    "    # 3. Corrected Voltage Magnitude\n",
    "    voltage_mag = x_i[:, 2] + 1.0  # Convert deviation to absolute voltage\n",
    "    voltage_dev = torch.abs(voltage_mag - 1.0).unsqueeze(1)\n",
    "    \n",
    "    # Concatenate new node features\n",
    "    x_i = torch.cat([x_i, node_betweenness.unsqueeze(1), node_deg.unsqueeze(1), voltage_dev], dim=1)\n",
    "    \n",
    "    # =====================\n",
    "    # New Edge Features\n",
    "    # =====================\n",
    "    # 1. Edge Betweenness Centrality\n",
    "    edge_bc_dict = nx.edge_betweenness_centrality(G)\n",
    "    edge_bc = torch.tensor([edge_bc_dict.get(tuple(e.tolist()), 0.0) for e in edge_index_i.T], dtype=torch.float)\n",
    "\n",
    "    # 2. Load Percentage (P / lr)\n",
    "    P = edge_attr_i[:, 0]  # Active power\n",
    "    lr = edge_attr_i[:, 3]  # Line rating\n",
    "    load_pct = (P / (lr + 1e-8)).unsqueeze(1)  # Add epsilon to avoid division by zero\n",
    "    \n",
    "    # 3. Electrical Betweenness (simplified)\n",
    "    Q = edge_attr_i[:, 1]  # Reactive power\n",
    "    elec_betweenness = (torch.abs(P) + torch.abs(Q)).unsqueeze(1)\n",
    "    \n",
    "    # Concatenate new edge features\n",
    "    edge_attr_i = torch.cat([\n",
    "        edge_attr_i, \n",
    "        edge_bc.unsqueeze(1),\n",
    "        load_pct,\n",
    "        elec_betweenness\n",
    "    ], dim=1)\n",
    "    \n",
    "    # 4) Graph label (binary or multi-class)\n",
    "    y_i = data_flat.y[i]\n",
    "    \n",
    "    # 5) Build a new Data object\n",
    "    subgraph_i = Data(\n",
    "        x=x_i,\n",
    "        edge_index=edge_index_i,\n",
    "        edge_attr=edge_attr_i,\n",
    "        y=y_i.unsqueeze(0),  # Keep graph-level label if needed\n",
    "        edge_mask=edge_mask_i  # Add binary edge labels\n",
    "    )\n",
    "    \n",
    "    return subgraph_i\n",
    "\n",
    "# Test subgraph reconstruction\n",
    "i_test = 0\n",
    "subgraph_0 = get_subgraph(loaded_data[0], loaded_data[1], i_test)\n",
    "print(\"Subgraph 0:\")\n",
    "print(subgraph_0)\n",
    "print(\"Edge mask values:\", subgraph_0.edge_mask.unique())  # Should be [0., 1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Edge Mask Coverage\n",
      "Graphs with cascading failures: 2\n",
      "Graphs with defined edge_mask: 2\n",
      " Edge mask coverage is complete for cascading failure graphs.\n",
      "\n",
      " Checking Edge Label Distribution\n",
      "Graph 0: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 1: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Graph 2: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 3: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Total edges: 12\n",
      "Tripped edges (1s): 2\n",
      "Non-tripped edges (0s): 10\n",
      "Percentage of tripped edges: 16.67%\n",
      "\n",
      " Validating Graph-Edge Label Consistency \n",
      "Graph 0: Consistent - y=1 and tripped edges present.\n",
      "Graph 1: Consistent - y=0 and no tripped edges.\n",
      "Graph 2: Consistent - y=1 and tripped edges present.\n",
      "Graph 3: Consistent - y=0 and no tripped edges.\n",
      "All graphs have consistent edge_mask and y labels.\n"
     ]
    }
   ],
   "source": [
    "#Cell 3: Verification \n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "dataset = [\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 1, 0]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3),  # Category B or D\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 0, 1]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3)   # Category B or D\n",
    "]\n",
    "\n",
    "def verify_edge_mask_coverage(dataset):\n",
    "    \"\"\"Check if edge_mask is defined for all graphs with cascading failures (y=1).\"\"\"\n",
    "    print(\"Verifying Edge Mask Coverage\")\n",
    "    has_cascading = 0\n",
    "    has_edge_mask_defined = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        if graph.y.item() == 1:  # Graphs with cascading failures (Categories A and C)\n",
    "            has_cascading += 1\n",
    "            if graph.edge_mask is not None and len(graph.edge_mask) == graph.num_edges:\n",
    "                has_edge_mask_defined += 1\n",
    "            else:\n",
    "                print(f\"Graph {i}: Missing or incomplete edge_mask for cascading failure graph.\")\n",
    "    \n",
    "    print(f\"Graphs with cascading failures: {has_cascading}\")\n",
    "    print(f\"Graphs with defined edge_mask: {has_edge_mask_defined}\")\n",
    "    if has_cascading == has_edge_mask_defined:\n",
    "        print(\" Edge mask coverage is complete for cascading failure graphs.\")\n",
    "    else:\n",
    "        print(\"Edge mask is missing or incomplete for some cascading failure graphs.\")\n",
    "\n",
    "def check_edge_label_distribution(dataset):\n",
    "    \"\"\"Examine the distribution of edge labels (1s and 0s) across graphs.\"\"\"\n",
    "    print(\"\\n Checking Edge Label Distribution\")\n",
    "    total_edges = 0\n",
    "    tripped_edges = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        num_tripped = edge_mask.sum().item()\n",
    "        total_edges += len(edge_mask)\n",
    "        tripped_edges += num_tripped\n",
    "        print(f\"Graph {i}: {num_tripped} tripped edges (1s), {len(edge_mask) - num_tripped} non-tripped (0s)\")\n",
    "    \n",
    "    print(f\"Total edges: {total_edges}\")\n",
    "    print(f\"Tripped edges (1s): {tripped_edges}\")\n",
    "    print(f\"Non-tripped edges (0s): {total_edges - tripped_edges}\")\n",
    "    print(f\"Percentage of tripped edges: {(tripped_edges / total_edges * 100):.2f}%\")\n",
    "\n",
    "def validate_graph_edge_consistency(dataset):\n",
    "    \"\"\"Ensure edge_mask aligns with graph-level labels (y).\"\"\"\n",
    "    print(\"\\n Validating Graph-Edge Label Consistency \")\n",
    "    all_valid = True\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        y = graph.y.item()\n",
    "        \n",
    "        if y == 1:  # Categories A and C (cascading failures)\n",
    "            if edge_mask.sum() == 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=1 but no tripped edges in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=1 and tripped edges present.\")\n",
    "        elif y == 0:  # Categories B and D (no cascading failures)\n",
    "            if edge_mask.sum() > 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=0 but tripped edges present in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=0 and no tripped edges.\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"All graphs have consistent edge_mask and y labels.\")\n",
    "    else:\n",
    "        print(\" Some graphs have inconsistencies between edge_mask and y.\")\n",
    "\n",
    "# Run the verifications\n",
    "verify_edge_mask_coverage(dataset)\n",
    "check_edge_label_distribution(dataset)\n",
    "validate_graph_edge_consistency(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subgraphs in full_dataset: 3444\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create a PyTorch Dataset for our subgraphs\n",
    "\n",
    "class PowerGraphDataset(Dataset):\n",
    "    def __init__(self, data_flat, meta_dict, indices=None, filter_category_A=True):\n",
    "        \"\"\"\n",
    "        data_flat:  The giant flattened Data object\n",
    "        meta_dict:  Dictionary of offsets\n",
    "        indices:    Subgraph indices to include\n",
    "        filter_category_A: If True, only include graphs with cascading failures (edge_mask != 0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_flat = data_flat\n",
    "        self.meta_dict = meta_dict\n",
    "        self.filter_category_A = filter_category_A\n",
    "        \n",
    "        if indices is None:\n",
    "            # Default to all graphs (0 to num_subgraphs-1)\n",
    "            self.indices = range(len(meta_dict['x']) - 1)\n",
    "        else:\n",
    "            self.indices = indices\n",
    "        \n",
    "        # Filter to Category A (DNS > 0 with cascading failures)\n",
    "        if self.filter_category_A:\n",
    "            self.indices = self._filter_category_A()\n",
    "    \n",
    "    def _filter_category_A(self):\n",
    "        \"\"\"Retain indices where edge_mask has at least one failure (1)\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in self.indices:\n",
    "            e_start = self.meta_dict['edge_index'][idx].item()\n",
    "            e_end = self.meta_dict['edge_index'][idx+1].item()\n",
    "            edge_mask = self.data_flat.edge_mask[e_start:e_end]  # Use edge_mask\n",
    "            if edge_mask.sum() > 0:  # At least one failed edge\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subgraph_id = self.indices[idx]\n",
    "        return get_subgraph(self.data_flat, self.meta_dict, subgraph_id)\n",
    "\n",
    "# Create dataset (only Category A graphs)\n",
    "full_dataset = PowerGraphDataset(loaded_data[0], loaded_data[1], filter_category_A=True)\n",
    "print(\"Total subgraphs in full_dataset:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge label distribution:\n",
      "- Failed edges (1): 8252.0 (3.25%)\n",
      "- Stable edges (0): 245398.0 (96.75%)\n"
     ]
    }
   ],
   "source": [
    "# After creating full_dataset (Cell 5):\n",
    "all_edge_masks = torch.cat([batch.edge_mask for batch in full_dataset])\n",
    "num_positive = all_edge_masks.sum().item()\n",
    "num_negative = len(all_edge_masks) - num_positive\n",
    "\n",
    "print(f\"Edge label distribution:\")\n",
    "print(f\"- Failed edges (1): {num_positive} ({num_positive / len(all_edge_masks):.2%})\")\n",
    "print(f\"- Stable edges (0): {num_negative} ({num_negative / len(all_edge_masks):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2755\n",
      "Val set size:   344\n",
      "Test set size:  345\n",
      "DataLoaders created with batch_size = 32\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train/Val/Test split & DataLoaders (with class-aware splitting)\n",
    "\n",
    "# 1) Handle extreme class imbalance (3.25% positive edges)\n",
    "# --------------------------------------------------------\n",
    "# Calculate split sizes based on the filtered Category A dataset\n",
    "num_subgraphs = len(full_dataset)  # 3444 (from your output)\n",
    "train_size = int(0.8 * num_subgraphs)   # ~2755\n",
    "val_size = int(0.1 * num_subgraphs)     # ~344\n",
    "test_size = num_subgraphs - train_size - val_size  # ~345\n",
    "\n",
    "# 2) Stratified split to preserve class distribution\n",
    "# (PyTorch's random_split doesn't stratify, so we use a custom approach)\n",
    "indices = np.arange(num_subgraphs)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx = indices[train_size:train_size+val_size]\n",
    "test_idx = indices[train_size+val_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_idx)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Val set size:   {len(val_dataset)}\")\n",
    "print(f\"Test set size:  {len(test_dataset)}\")\n",
    "\n",
    "# 3) Build PyG DataLoaders with class-aware sampling\n",
    "batch_size = 32\n",
    "\n",
    "# Use weighted sampler to handle edge-level imbalance\n",
    "graph_weights = [batch.edge_mask.float().mean().item() for batch in full_dataset]  # Proportion of positive edges per graph\n",
    "train_sample_weights = [graph_weights[i] for i in train_idx]\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    train_sample_weights, len(train_idx), replacement=True\n",
    ")\n",
    "\n",
    "train_loader = PyGDataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with batch_size =\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Modularizing to 03 Models ###########\n",
    "\n",
    "# Run the previous cells untill cell 6 \n",
    "\n",
    "# Cell 7: Model Architectures\n",
    "\n",
    "\n",
    "class GINEBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        self.gnn_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        self.conv = GINEConv(nn=self.gnn_mlp, edge_dim=in_channels_edge)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = self.conv(h, edge_index, edge_attr)\n",
    "        h_u = h[edge_index[0]]\n",
    "        h_v = h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], 1)).squeeze()\n",
    "\n",
    "class GATBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        self.conv = GATConv(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = self.conv(h, edge_index)\n",
    "        h_u = h[edge_index[0]]\n",
    "        h_v = h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], 1)).squeeze()\n",
    "\n",
    "class GraphConvBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=3, in_channels_edge=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        self.conv = GraphConv(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = self.conv(h, edge_index)\n",
    "        h_u = h[edge_index[0]]\n",
    "        h_v = h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Framework\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "    total_edges = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_labels = batch.edge_mask.float()\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(logits, edge_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Metrics\n",
    "            preds = (torch.sigmoid(logits) > 0.4).long()\n",
    "            prec, rec, f1 = calculate_metrics(preds, edge_labels.long())\n",
    "            \n",
    "            metrics['precision'] += prec * edge_labels.numel()\n",
    "            metrics['recall'] += rec * edge_labels.numel()\n",
    "            metrics['f1'] += f1 * edge_labels.numel()\n",
    "            total_edges += edge_labels.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    for key in metrics:\n",
    "        metrics[key] /= total_edges\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "def train_and_evaluate_model(model, model_name, train_loader, val_loader, device, num_epochs=5):\n",
    "    criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    results = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss = criterion(logits, batch.edge_mask.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Save results\n",
    "        results['train_loss'].append(total_train_loss/len(train_loader))\n",
    "        results['val_loss'].append(val_loss)\n",
    "        results['precision'].append(val_metrics['precision'])\n",
    "        results['recall'].append(val_metrics['recall'])\n",
    "        results['f1'].append(val_metrics['f1'])\n",
    "        \n",
    "        print(f\"{model_name} - Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {results['train_loss'][-1]:.4f}\")\n",
    "        print(f\"  Val Loss: {results['val_loss'][-1]:.4f}\")\n",
    "        print(f\"  Val F1: {results['f1'][-1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results_dict, filename=\"model_results.json\"):\n",
    "    # Convert tensors to Python floats\n",
    "    for model in results_dict:\n",
    "        for metric in results_dict[model]:\n",
    "            results_dict[model][metric] = [float(v) for v in results_dict[model][metric]]\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {Path(filename).absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "Training GINE Model\n",
      "========================================\n",
      "GINE - Epoch 1/5\n",
      "  Train Loss: 0.0472\n",
      "  Val Loss: 0.0259\n",
      "  Val F1: 0.0000\n",
      "--------------------------------------------------\n",
      "GINE - Epoch 2/5\n",
      "  Train Loss: 0.0241\n",
      "  Val Loss: 0.0127\n",
      "  Val F1: 0.8576\n",
      "--------------------------------------------------\n",
      "GINE - Epoch 3/5\n",
      "  Train Loss: 0.0166\n",
      "  Val Loss: 0.0110\n",
      "  Val F1: 0.8612\n",
      "--------------------------------------------------\n",
      "GINE - Epoch 4/5\n",
      "  Train Loss: 0.0149\n",
      "  Val Loss: 0.0095\n",
      "  Val F1: 0.8541\n",
      "--------------------------------------------------\n",
      "GINE - Epoch 5/5\n",
      "  Train Loss: 0.0132\n",
      "  Val Loss: 0.0085\n",
      "  Val F1: 0.8418\n",
      "--------------------------------------------------\n",
      "\n",
      "========================================\n",
      "Training GAT Model\n",
      "========================================\n",
      "GAT - Epoch 1/5\n",
      "  Train Loss: 0.0380\n",
      "  Val Loss: 0.0272\n",
      "  Val F1: 0.0000\n",
      "--------------------------------------------------\n",
      "GAT - Epoch 2/5\n",
      "  Train Loss: 0.0280\n",
      "  Val Loss: 0.0184\n",
      "  Val F1: 0.2284\n",
      "--------------------------------------------------\n",
      "GAT - Epoch 3/5\n",
      "  Train Loss: 0.0204\n",
      "  Val Loss: 0.0130\n",
      "  Val F1: 0.8428\n",
      "--------------------------------------------------\n",
      "GAT - Epoch 4/5\n",
      "  Train Loss: 0.0172\n",
      "  Val Loss: 0.0114\n",
      "  Val F1: 0.8352\n",
      "--------------------------------------------------\n",
      "GAT - Epoch 5/5\n",
      "  Train Loss: 0.0150\n",
      "  Val Loss: 0.0107\n",
      "  Val F1: 0.7961\n",
      "--------------------------------------------------\n",
      "\n",
      "========================================\n",
      "Training GraphConv Model\n",
      "========================================\n",
      "GraphConv - Epoch 1/5\n",
      "  Train Loss: 0.0341\n",
      "  Val Loss: 0.0246\n",
      "  Val F1: 0.0000\n",
      "--------------------------------------------------\n",
      "GraphConv - Epoch 2/5\n",
      "  Train Loss: 0.0244\n",
      "  Val Loss: 0.0142\n",
      "  Val F1: 0.8347\n",
      "--------------------------------------------------\n",
      "GraphConv - Epoch 3/5\n",
      "  Train Loss: 0.0171\n",
      "  Val Loss: 0.0110\n",
      "  Val F1: 0.7942\n",
      "--------------------------------------------------\n",
      "GraphConv - Epoch 4/5\n",
      "  Train Loss: 0.0155\n",
      "  Val Loss: 0.0189\n",
      "  Val F1: 0.3917\n",
      "--------------------------------------------------\n",
      "GraphConv - Epoch 5/5\n",
      "  Train Loss: 0.0144\n",
      "  Val Loss: 0.0101\n",
      "  Val F1: 0.7651\n",
      "--------------------------------------------------\n",
      "Results saved to /mmfs1/home/muhammad.kazim/PowerGraph-Graph/model_results.json\n",
      "\n",
      "Final Comparison with base features:\n",
      "GINE:\n",
      "  Best Val F1: 0.8612\n",
      "  Final Val F1: 0.8418\n",
      "  Precision/Recall: 0.8338/0.8531\n",
      "GAT:\n",
      "  Best Val F1: 0.8428\n",
      "  Final Val F1: 0.7961\n",
      "  Precision/Recall: 0.7691/0.8311\n",
      "GraphConv:\n",
      "  Best Val F1: 0.8347\n",
      "  Final Val F1: 0.7651\n",
      "  Precision/Recall: 0.7112/0.8312\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Model Comparison\n",
    "\n",
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for positive class\n",
    "        self.gamma = gamma  # Focuses on hard examples\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)  # pt = p if label=1, 1-p otherwise\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * bce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"GINE\": GINEBasedClassifier(\n",
    "        in_channels_node=6,  # Original 3 + 3 new\n",
    "        in_channels_edge=7   # Original 4 + 3 new\n",
    "    ).to(device),\n",
    "    \"GAT\": GATBasedClassifier(\n",
    "        in_channels_node=6,\n",
    "        in_channels_edge=7\n",
    "    ).to(device),\n",
    "    \"GraphConv\": GraphConvBasedClassifier(\n",
    "        in_channels_node=6,\n",
    "        in_channels_edge=7\n",
    "    ).to(device)\n",
    "}\n",
    "\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)  # alpha >0.5 to emphasize positives\n",
    "\n",
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "def calculate_metrics(preds, labels):\n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    precision = TP / (TP + FP + 1e-8)  # Avoid division by zero\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} Model\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    results = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=5\n",
    "    )\n",
    "    \n",
    "    all_results[model_name] = results\n",
    "\n",
    "# Save results to file\n",
    "save_results(all_results)\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\nFinal Comparison with enhanced features:\")\n",
    "for model_name, results in all_results.items():\n",
    "    best_f1 = max(results['f1'])\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Best Val F1: {best_f1:.4f}\")\n",
    "    print(f\"  Final Val F1: {results['f1'][-1]:.4f}\")\n",
    "    print(f\"  Precision/Recall: {results['precision'][-1]:.4f}/{results['recall'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores:\n",
      " [Original Features] P, Q, X, lr | [New] Edge BC, Load %, Elec BC\n",
      "[0.16637447 0.17915879 0.1656326  0.18089306 0.26644358 0.82438076\n",
      " 0.16416733 0.1736131  0.51325274 0.17572105 0.18103446 0.18071322\n",
      " 0.48833895 0.17294693 0.17571306 0.1799339  0.14567488 0.0315374\n",
      " 0.1828467  0.1699042  0.21468116 0.16592245 0.17001054 0.18492217\n",
      " 0.0453057  0.37820315 0.1648946  0.1659217  0.16850363 0.18119232\n",
      " 0.4076903  0.1257217  0.16627455 0.17905597 0.16553256 0.18078971\n",
      " 0.26328215 0.8267618  0.16406792 0.1735108  0.51581836 0.17561916\n",
      " 0.18093115 0.177869   0.48612732 0.17284486 0.17561093 0.1798305\n",
      " 0.14649981 0.03363338 0.18274307 0.16980328 0.21818788 0.16582264\n",
      " 0.16990957 0.18481795 0.04746782 0.3805976  0.16479482 0.16582184\n",
      " 0.16621585 0.18108885 0.4109217  0.12025373 0.01301535 0.00942705\n",
      " 0.02561018 0.04436544 0.01560618 0.10379489 0.07513484]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Feature Importance Analysis (Example)\n",
    "\n",
    "def analyze_feature_importance(model, loader, device):\n",
    "    \"\"\"Track feature gradients to estimate importance\"\"\"\n",
    "    model.eval()\n",
    "    feature_grads = torch.zeros(model.edge_mlp[0].in_features).to(device)\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, batch.edge_mask.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients from first MLP layer\n",
    "        feature_grads += model.edge_mlp[0].weight.grad.abs().sum(dim=0)\n",
    "    \n",
    "    # Normalize\n",
    "    feature_grads /= len(loader.dataset)\n",
    "    print(\"Feature Importance Scores:\")\n",
    "    print(\" [Original Features] P, Q, X, lr | [New] Edge BC, Load %, Elec BC\")\n",
    "    print(feature_grads.cpu().numpy())\n",
    "\n",
    "# Run analysis\n",
    "analyze_feature_importance(models[\"GINE\"], train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance for GINE ===\n",
      "\n",
      "=== Feature Importance for GAT ===\n",
      "\n",
      "=== Feature Importance for GraphConv ===\n",
      "\n",
      "Feature Importance Results saved to 'feature_importance_results.csv'.\n",
      "Model                   GINE       GAT  GraphConv\n",
      "Feature                                          \n",
      "P (Active Power)    1.154666  2.117865   0.107767\n",
      "Q (Reactive Power)  1.243408  0.473009   3.750725\n",
      "X (Impedance)       1.149516  4.266479   1.806939\n",
      "lr (Line Rating)    1.255445  3.786949   4.867756\n",
      "Edge BC             1.847913  1.463134   3.827049\n",
      "...                      ...       ...        ...\n",
      "Extra Feature 60    0.174633  0.130924   0.055824\n",
      "Extra Feature 61    0.303501  0.351021   0.151869\n",
      "Extra Feature 62    0.108771  0.153942   0.086767\n",
      "Extra Feature 63    0.720886  0.485142   0.404887\n",
      "Extra Feature 64    0.524252  0.726933   0.405582\n",
      "\n",
      "[71 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_feature_importance(model, loader, device):\n",
    "    \"\"\"Track feature gradients to estimate importance.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dynamically determine feature count\n",
    "    num_features = model.edge_mlp[0].in_features\n",
    "    feature_grads = torch.zeros(num_features).to(device)\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, batch.edge_mask.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients from first MLP layer\n",
    "        feature_grads += model.edge_mlp[0].weight.grad.abs().sum(dim=0)\n",
    "    \n",
    "    # Normalize\n",
    "    feature_grads /= len(loader.dataset)\n",
    "    return feature_grads.cpu().numpy()\n",
    "\n",
    "# Store results in a dictionary\n",
    "feature_importance_results = {}\n",
    "\n",
    "# Analyze feature importance for all models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== Feature Importance for {model_name} ===\")\n",
    "    feature_scores = analyze_feature_importance(model, train_loader, device)\n",
    "    \n",
    "    # Store scores in dictionary\n",
    "    feature_importance_results[model_name] = feature_scores\n",
    "\n",
    "# Dynamically generate feature names based on actual feature count\n",
    "num_features = len(next(iter(feature_importance_results.values())))\n",
    "base_features = [\"P (Active Power)\", \"Q (Reactive Power)\", \"X (Impedance)\", \"lr (Line Rating)\"]\n",
    "new_features = [\"Edge BC\", \"Load %\", \"Elec BC\"]\n",
    "feature_names = base_features + new_features\n",
    "\n",
    "# If there are unexpected additional features, append generic labels\n",
    "if len(feature_names) < num_features:\n",
    "    extra_features = [f\"Extra Feature {i+1}\" for i in range(num_features - len(feature_names))]\n",
    "    feature_names += extra_features\n",
    "\n",
    "# Format results in a readable table\n",
    "df_importance = pd.DataFrame(feature_importance_results, index=feature_names[:num_features])\n",
    "df_importance.index.name = \"Feature\"\n",
    "df_importance.columns.name = \"Model\"\n",
    "\n",
    "# Save to CSV\n",
    "df_importance.to_csv(\"feature_importance_results.csv\")\n",
    "print(\"\\nFeature Importance Results saved to 'feature_importance_results.csv'.\")\n",
    "\n",
    "# Print table\n",
    "print(df_importance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
