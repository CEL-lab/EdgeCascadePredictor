{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'tuple'>\n",
      "Length of dataset tuple: 2\n",
      "\n",
      "First element type: <class 'torch_geometric.data.data.Data'>\n",
      "\n",
      "Metadata keys: ['x', 'edge_index', 'edge_attr', 'y', 'edge_mask', 'idx']\n",
      "x: <class 'torch.Tensor'>, first 10: tensor([  0,  29,  58,  87, 116, 145, 174, 203, 232, 261])\n",
      "edge_index: <class 'torch.Tensor'>, first 10: tensor([   0,  196,  392,  588,  784,  980, 1176, 1372, 1568, 1764])\n",
      "edge_attr: <class 'torch.Tensor'>, first 10: tensor([   0,  196,  392,  588,  784,  980, 1176, 1372, 1568, 1764])\n",
      "y: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "edge_mask: <class 'torch.Tensor'>, first 10: tensor([   0,  196,  392,  588,  784,  980, 1176, 1372, 1568, 1764])\n",
      "idx: <class 'torch.Tensor'>, first 10: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.168027.bright04/ipykernel_3204703/1934631425.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(file_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Basic Imports and Data Loading\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GINEConv, GATConv, GraphConv\n",
    "\n",
    "# Load dataset\n",
    "#file_path = 'dataset/ieee118/processed_b/data.pt'\n",
    "#file_path = 'dataset/ieee24/ieee24/processed_b/data.pt'\n",
    "#file_path = 'dataset/ieee39/processed_b/data.pt'\n",
    "file_path = 'dataset/uk/processed_b/data.pt'\n",
    "\n",
    "loaded_data = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Loaded data type:\", type(loaded_data))\n",
    "print(\"Length of dataset tuple:\", len(loaded_data))\n",
    "\n",
    "# Inspect first element (summary only)\n",
    "print(\"\\nFirst element type:\", type(loaded_data[0]))\n",
    "\n",
    "# Extract metadata dictionary\n",
    "metadata_dict = loaded_data[1]\n",
    "print(\"\\nMetadata keys:\", list(metadata_dict.keys()))\n",
    "\n",
    "# Preview metadata values (first 10 entries)\n",
    "for key, val in metadata_dict.items():\n",
    "    preview = val[:10] if hasattr(val, '__len__') else \"N/A\"\n",
    "    print(f\"{key}: {type(val)}, first 10: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph 0:\n",
      "Data(x=[29, 6], edge_index=[2, 196], edge_attr=[196, 7], y=[1, 1], edge_mask=[196])\n",
      "Edge mask values: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Enhanced get_subgraph with new features\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import degree, to_networkx\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def get_subgraph(data_flat, meta_dict, i):\n",
    "    \"\"\"\n",
    "    Enhanced function to reconstruct the i-th subgraph with additional features.\n",
    "\n",
    "    data_flat: The big flattened Data object (loaded_data[0])\n",
    "    meta_dict: The dictionary of offsets (loaded_data[1])\n",
    "    i        : Index of the subgraph we want to reconstruct\n",
    "\n",
    "    returns: a PyG Data object representing the i-th subgraph\n",
    "    \"\"\"\n",
    "    # 1) Node offsets\n",
    "    x_start = meta_dict['x'][i].item()\n",
    "    x_end   = meta_dict['x'][i+1].item()\n",
    "    x_i = data_flat.x[x_start:x_end]\n",
    "    \n",
    "    # 2) Edge offsets\n",
    "    e_start = meta_dict['edge_index'][i].item()\n",
    "    e_end   = meta_dict['edge_index'][i+1].item()\n",
    "    edge_index_i = data_flat.edge_index[:, e_start:e_end]\n",
    "    edge_attr_i  = data_flat.edge_attr[e_start:e_end]\n",
    "    \n",
    "    # 3) Load TRUE binary edge labels (explanation_mask)\n",
    "    edge_mask_i = data_flat.edge_mask[e_start:e_end].float()\n",
    "    \n",
    "    # Strict validation for edge_mask\n",
    "    if not torch.all(torch.isin(edge_mask_i, torch.tensor([0., 1.]))):\n",
    "        print(f\"BAD SUBGRAPH {i}:\")\n",
    "        print(\"Unique values:\", edge_mask_i.unique())\n",
    "        print(\"Edge indices:\", edge_index_i)\n",
    "        raise ValueError(\"Edge mask contains non-binary values\")\n",
    "    \n",
    "    # =====================\n",
    "    # New Node Features\n",
    "    # =====================\n",
    "    # Convert PyG edge index to NetworkX graph\n",
    "    G = to_networkx(Data(edge_index=edge_index_i, num_nodes=x_i.shape[0]), to_undirected=True)\n",
    "\n",
    "    # 1. Node Betweenness Centrality\n",
    "    node_betweenness_dict = nx.betweenness_centrality(G)\n",
    "    node_betweenness = torch.tensor([node_betweenness_dict.get(n, 0.0) for n in range(x_i.shape[0])], dtype=torch.float)\n",
    "\n",
    "    # 2. Node Degree (Fixed `.reshape(-1)`)\n",
    "    node_deg = degree(edge_index_i.reshape(-1), num_nodes=x_i.shape[0], dtype=torch.float)\n",
    "    \n",
    "    # 3. Corrected Voltage Magnitude\n",
    "    voltage_mag = x_i[:, 2] + 1.0  # Convert deviation to absolute voltage\n",
    "    voltage_dev = torch.abs(voltage_mag - 1.0).unsqueeze(1)\n",
    "    \n",
    "    # Concatenate new node features\n",
    "    x_i = torch.cat([x_i, node_betweenness.unsqueeze(1), node_deg.unsqueeze(1), voltage_dev], dim=1)\n",
    "    \n",
    "    # =====================\n",
    "    # New Edge Features\n",
    "    # =====================\n",
    "    # 1. Edge Betweenness Centrality\n",
    "    edge_bc_dict = nx.edge_betweenness_centrality(G)\n",
    "    #edge_bc = torch.tensor([edge_bc_dict.get(tuple(e.tolist()), 0.0) for e in edge_index_i.T], dtype=torch.float)\n",
    "    edge_bc = torch.tensor([edge_bc_dict.get(tuple(sorted(e.tolist())), 0.0)\n",
    "                        for e in edge_index_i.T], dtype=torch.float)\n",
    "\n",
    "\n",
    "    # 2. Load Percentage (P / lr)\n",
    "    P = edge_attr_i[:, 0]  # Active power\n",
    "    lr = edge_attr_i[:, 3]  # Line rating\n",
    "    load_pct = (P / (lr + 1e-8)).unsqueeze(1)  # Add epsilon to avoid division by zero\n",
    "    \n",
    "    # 3. Electrical Betweenness (simplified)\n",
    "    Q = edge_attr_i[:, 1]  # Reactive power\n",
    "    elec_betweenness = (torch.abs(P) + torch.abs(Q)).unsqueeze(1)\n",
    "    \n",
    "    # Concatenate new edge features\n",
    "    edge_attr_i = torch.cat([\n",
    "        edge_attr_i, \n",
    "        edge_bc.unsqueeze(1),\n",
    "        load_pct,\n",
    "        elec_betweenness\n",
    "    ], dim=1)\n",
    "    \n",
    "    # 4) Graph label (binary or multi-class)\n",
    "    y_i = data_flat.y[i]\n",
    "    \n",
    "    # 5) Build a new Data object\n",
    "    subgraph_i = Data(\n",
    "        x=x_i,\n",
    "        edge_index=edge_index_i,\n",
    "        edge_attr=edge_attr_i,\n",
    "        y=y_i.unsqueeze(0),  # Keep graph-level label if needed\n",
    "        edge_mask=edge_mask_i  # Add binary edge labels\n",
    "    )\n",
    "    \n",
    "    return subgraph_i\n",
    "\n",
    "# Test subgraph reconstruction\n",
    "i_test = 0\n",
    "subgraph_0 = get_subgraph(loaded_data[0], loaded_data[1], i_test)\n",
    "print(\"Subgraph 0:\")\n",
    "print(subgraph_0)\n",
    "print(\"Edge mask values:\", subgraph_0.edge_mask.unique())  # Should be [0., 1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Edge Mask Coverage\n",
      "Graphs with cascading failures: 2\n",
      "Graphs with defined edge_mask: 2\n",
      " Edge mask coverage is complete for cascading failure graphs.\n",
      "\n",
      " Checking Edge Label Distribution\n",
      "Graph 0: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 1: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Graph 2: 1 tripped edges (1s), 2 non-tripped (0s)\n",
      "Graph 3: 0 tripped edges (1s), 3 non-tripped (0s)\n",
      "Total edges: 12\n",
      "Tripped edges (1s): 2\n",
      "Non-tripped edges (0s): 10\n",
      "Percentage of tripped edges: 16.67%\n",
      "\n",
      " Validating Graph-Edge Label Consistency \n",
      "Graph 0: Consistent - y=1 and tripped edges present.\n",
      "Graph 1: Consistent - y=0 and no tripped edges.\n",
      "Graph 2: Consistent - y=1 and tripped edges present.\n",
      "Graph 3: Consistent - y=0 and no tripped edges.\n",
      "All graphs have consistent edge_mask and y labels.\n"
     ]
    }
   ],
   "source": [
    "#Cell 3: Verification \n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "dataset = [\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 1, 0]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3),  # Category B or D\n",
    "    Data(y=torch.tensor(1), edge_mask=torch.tensor([0, 0, 1]), num_edges=3),  # Category A or C\n",
    "    Data(y=torch.tensor(0), edge_mask=torch.tensor([0, 0, 0]), num_edges=3)   # Category B or D\n",
    "]\n",
    "\n",
    "def verify_edge_mask_coverage(dataset):\n",
    "    \"\"\"Check if edge_mask is defined for all graphs with cascading failures (y=1).\"\"\"\n",
    "    print(\"Verifying Edge Mask Coverage\")\n",
    "    has_cascading = 0\n",
    "    has_edge_mask_defined = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        if graph.y.item() == 1:  # Graphs with cascading failures (Categories A and C)\n",
    "            has_cascading += 1\n",
    "            if graph.edge_mask is not None and len(graph.edge_mask) == graph.num_edges:\n",
    "                has_edge_mask_defined += 1\n",
    "            else:\n",
    "                print(f\"Graph {i}: Missing or incomplete edge_mask for cascading failure graph.\")\n",
    "    \n",
    "    print(f\"Graphs with cascading failures: {has_cascading}\")\n",
    "    print(f\"Graphs with defined edge_mask: {has_edge_mask_defined}\")\n",
    "    if has_cascading == has_edge_mask_defined:\n",
    "        print(\" Edge mask coverage is complete for cascading failure graphs.\")\n",
    "    else:\n",
    "        print(\"Edge mask is missing or incomplete for some cascading failure graphs.\")\n",
    "\n",
    "def check_edge_label_distribution(dataset):\n",
    "    \"\"\"Examine the distribution of edge labels (1s and 0s) across graphs.\"\"\"\n",
    "    print(\"\\n Checking Edge Label Distribution\")\n",
    "    total_edges = 0\n",
    "    tripped_edges = 0\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        num_tripped = edge_mask.sum().item()\n",
    "        total_edges += len(edge_mask)\n",
    "        tripped_edges += num_tripped\n",
    "        print(f\"Graph {i}: {num_tripped} tripped edges (1s), {len(edge_mask) - num_tripped} non-tripped (0s)\")\n",
    "    \n",
    "    print(f\"Total edges: {total_edges}\")\n",
    "    print(f\"Tripped edges (1s): {tripped_edges}\")\n",
    "    print(f\"Non-tripped edges (0s): {total_edges - tripped_edges}\")\n",
    "    print(f\"Percentage of tripped edges: {(tripped_edges / total_edges * 100):.2f}%\")\n",
    "\n",
    "def validate_graph_edge_consistency(dataset):\n",
    "    \"\"\"Ensure edge_mask aligns with graph-level labels (y).\"\"\"\n",
    "    print(\"\\n Validating Graph-Edge Label Consistency \")\n",
    "    all_valid = True\n",
    "    \n",
    "    for i, graph in enumerate(dataset):\n",
    "        edge_mask = graph.edge_mask\n",
    "        y = graph.y.item()\n",
    "        \n",
    "        if y == 1:  # Categories A and C (cascading failures)\n",
    "            if edge_mask.sum() == 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=1 but no tripped edges in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=1 and tripped edges present.\")\n",
    "        elif y == 0:  # Categories B and D (no cascading failures)\n",
    "            if edge_mask.sum() > 0:\n",
    "                print(f\"Graph {i}: Inconsistent - y=0 but tripped edges present in edge_mask.\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                print(f\"Graph {i}: Consistent - y=0 and no tripped edges.\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"All graphs have consistent edge_mask and y labels.\")\n",
    "    else:\n",
    "        print(\" Some graphs have inconsistencies between edge_mask and y.\")\n",
    "\n",
    "# Run the verifications\n",
    "verify_edge_mask_coverage(dataset)\n",
    "check_edge_label_distribution(dataset)\n",
    "validate_graph_edge_consistency(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subgraphs in full_dataset: 4531\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create a PyTorch Dataset for our subgraphs\n",
    "\n",
    "class PowerGraphDataset(Dataset):\n",
    "    def __init__(self, data_flat, meta_dict, indices=None, filter_category_A=True):\n",
    "        \"\"\"\n",
    "        data_flat:  The giant flattened Data object\n",
    "        meta_dict:  Dictionary of offsets\n",
    "        indices:    Subgraph indices to include\n",
    "        filter_category_A: If True, only include graphs with cascading failures (edge_mask != 0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_flat = data_flat\n",
    "        self.meta_dict = meta_dict\n",
    "        self.filter_category_A = filter_category_A\n",
    "        \n",
    "        if indices is None:\n",
    "            self.indices = list(range(len(meta_dict['x']) - 1))\n",
    "        else:\n",
    "            self.indices = indices\n",
    "            \n",
    "        # Filter to Category A (DNS > 0 with cascading failures)\n",
    "        if self.filter_category_A:\n",
    "            self.indices = self._filter_category_A()\n",
    "    \n",
    "    def _filter_category_A(self):\n",
    "        \"\"\"Retain indices where edge_mask has at least one failure (1)\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in self.indices:\n",
    "            e_start = self.meta_dict['edge_index'][idx].item()\n",
    "            e_end = self.meta_dict['edge_index'][idx+1].item()\n",
    "            edge_mask = self.data_flat.edge_mask[e_start:e_end]  # Use edge_mask\n",
    "            if edge_mask.sum() > 0:  # At least one failed edge\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subgraph_id = self.indices[idx]\n",
    "        return get_subgraph(self.data_flat, self.meta_dict, subgraph_id)\n",
    "\n",
    "# Create dataset (only Category A graphs)\n",
    "full_dataset = PowerGraphDataset(loaded_data[0], loaded_data[1], filter_category_A=True)\n",
    "print(\"Total subgraphs in full_dataset:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge label distribution:\n",
      "- Failed edges (1): 28168.0 (3.18%)\n",
      "- Stable edges (0): 856874.0 (96.82%)\n"
     ]
    }
   ],
   "source": [
    "# After creating full_dataset (Cell 5):\n",
    "all_edge_masks = torch.cat([batch.edge_mask for batch in full_dataset])\n",
    "num_positive = all_edge_masks.sum().item()\n",
    "num_negative = len(all_edge_masks) - num_positive\n",
    "\n",
    "print(f\"Edge label distribution:\")\n",
    "print(f\"- Failed edges (1): {num_positive} ({num_positive / len(all_edge_masks):.2%})\")\n",
    "print(f\"- Stable edges (0): {num_negative} ({num_negative / len(all_edge_masks):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node‑feature ranges (col order = original 3  + betweenness, degree, |V−1|)\n",
      "            orig‑0:  min  -0.6415   max   0.5421   mean  -0.0000\n",
      "            orig‑1:  min  -0.4281   max   0.7893   mean   0.0000\n",
      "            orig‑2:  min  -0.9447   max   0.1346   mean   0.0000\n",
      "  node_betweenness:  min   0.0000   max   0.5105   mean   0.1095\n",
      "       node_degree:  min   2.0000   max  24.0000   mean  13.4711\n",
      "       voltage_dev:  min   0.0000   max   0.9447   mean   0.1130\n",
      "\n",
      "Edge‑feature ranges (col order = original 4 + edge_bc, load_pct, elec_betw)\n",
      "            orig‑0:  min  -0.6100   max   0.5772   mean  -0.0004\n",
      "            orig‑1:  min  -0.8540   max   0.5069   mean  -0.0003\n",
      "            orig‑2:  min  -0.1007   max   0.8993   mean  -0.0021\n",
      "            orig‑3:  min  -0.3198   max   0.6802   mean   0.0030\n",
      "           edge_bc:  min   0.0025   max   0.4680   mean   0.0802\n",
      "          load_pct:  min -21.5343   max  25.4076   mean   0.1954\n",
      "         elec_betw:  min   0.0012   max   1.2605   mean   0.2331\n"
     ]
    }
   ],
   "source": [
    "# 📍  Diagnostic Cell – feature range check\n",
    "import torch\n",
    "\n",
    "def feature_stats(tensor, name):\n",
    "    print(f\"{name:>18}:  min {tensor.min():>8.4f}   max {tensor.max():>8.4f}   mean {tensor.mean():>8.4f}\")\n",
    "\n",
    "# ── gather all node features ───────────────────────────────────────\n",
    "all_nodes = torch.cat([g.x for g in full_dataset])          # [N_total, 6]\n",
    "print(\"\\nNode‑feature ranges (col order = original 3  + betweenness, degree, |V−1|)\")\n",
    "for i, fn in enumerate([\"orig‑0\", \"orig‑1\", \"orig‑2\",\n",
    "                        \"node_betweenness\", \"node_degree\", \"voltage_dev\"]):\n",
    "    feature_stats(all_nodes[:, i], fn)\n",
    "\n",
    "# ── gather all edge features ───────────────────────────────────────\n",
    "all_edges = torch.cat([g.edge_attr for g in full_dataset])  # [E_total, 7]\n",
    "print(\"\\nEdge‑feature ranges (col order = original 4 + edge_bc, load_pct, elec_betw)\")\n",
    "for i, fn in enumerate([\"orig‑0\", \"orig‑1\", \"orig‑2\", \"orig‑3\",\n",
    "                        \"edge_bc\", \"load_pct\", \"elec_betw\"]):\n",
    "    feature_stats(all_edges[:, i], fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3624\n",
      "Val set size:   453\n",
      "Test set size:  454\n",
      "DataLoaders created with batch_size = 128\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train/Val/Test split & DataLoaders (with class-aware splitting)\n",
    "\n",
    "# 1) Handle extreme class imbalance\n",
    "# --------------------------------------------------------\n",
    "# Calculate split sizes based on the filtered Category A dataset\n",
    "num_subgraphs = len(full_dataset)  \n",
    "train_size = int(0.8 * num_subgraphs)   \n",
    "val_size = int(0.1 * num_subgraphs)     \n",
    "test_size = num_subgraphs - train_size - val_size  \n",
    "\n",
    "# 2) Stratified split to preserve class distribution\n",
    "indices = np.arange(num_subgraphs)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx = indices[train_size:train_size+val_size]\n",
    "test_idx = indices[train_size+val_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_idx)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Val set size:   {len(val_dataset)}\")\n",
    "print(f\"Test set size:  {len(test_dataset)}\")\n",
    "\n",
    "# 3) Build PyG DataLoaders with class-aware sampling\n",
    "batch_size = 128\n",
    "\n",
    "# Use weighted sampler to handle edge-level imbalance with inverse weighting\n",
    "graph_pos_frac  = [g.edge_mask.float().mean().item() for g in full_dataset]\n",
    "graph_weights   = [1.0 / (p if p > 0 else 1e-4) for p in graph_pos_frac]   # inverse proportion\n",
    "train_weights   = [graph_weights[i] for i in train_idx]\n",
    "train_sampler   = torch.utils.data.WeightedRandomSampler(train_weights,\n",
    "                                                         len(train_idx),\n",
    "                                                         replacement=True)\n",
    "\n",
    "train_loader = PyGDataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    ")\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with batch_size =\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight = 5.5  (used by BCEWithLogitsLoss)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6‑bis – compute pos_weight once (run AFTER Cell 6)\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ─────────────────────────────────────────────────────────\n",
    "#   Needed globally for class‑balanced BCE loss\n",
    "# ─────────────────────────────────────────────────────────\n",
    "all_edge_masks = torch.cat([g.edge_mask for g in full_dataset])\n",
    "num_pos = all_edge_masks.sum().item()\n",
    "num_neg = len(all_edge_masks) - num_pos\n",
    "pos_weight = torch.tensor([(num_neg / num_pos) ** 0.5], device=device) \n",
    "\n",
    "print(f\"pos_weight = {pos_weight.item():.1f}  (used by BCEWithLogitsLoss)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────\n",
    "# 📍 Cell 7 – Model Architectures\n",
    "# ────────────────────────────────\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log\n",
    "from torch_geometric.nn import GINEConv, GATConv, NNConv\n",
    "\n",
    "# ---- helpers ----------------------------------------------------\n",
    "BIAS_INIT = -log(pos_weight.item())      #  –ln(√(neg/pos))\n",
    "\n",
    "def make_input_norm(n_feats):\n",
    "    return nn.LayerNorm(n_feats, elementwise_affine=True)\n",
    "\n",
    "# ---- GINE -------------------------------------------------------\n",
    "class GINEBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=6, in_channels_edge=7, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # input normalisation\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        self.fc_in   = nn.Linear(in_channels_node, hidden_dim)\n",
    "        mlp          = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.LayerNorm(hidden_dim))\n",
    "        self.conv1   = GINEConv(nn=mlp, edge_dim=in_channels_edge)\n",
    "        self.conv2   = GINEConv(nn=mlp, edge_dim=in_channels_edge)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index, edge_attr))\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n",
    "\n",
    "\n",
    "# ---- GAT --------------------------------------------------------\n",
    "class GATBasedClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=6, in_channels_edge=7, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "        self.conv1 = GATConv(hidden_dim, hidden_dim // 4, heads=4, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim // 4, heads=4, concat=True)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index))\n",
    "        h = self.conv2(h, edge_index)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n",
    "\n",
    "\n",
    "# ---- Edge‑aware GraphConv (NNConv) ------------------------------\n",
    "class EdgeAwareGraphConvClassifier(nn.Module):\n",
    "    def __init__(self, in_channels_node=6, in_channels_edge=7, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.node_norm = make_input_norm(in_channels_node)\n",
    "        self.edge_norm = make_input_norm(in_channels_edge)\n",
    "\n",
    "        self.fc_in = nn.Linear(in_channels_node, hidden_dim)\n",
    "\n",
    "        edge_net = nn.Sequential(\n",
    "            nn.Linear(in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        self.conv1 = NNConv(hidden_dim, hidden_dim, edge_net)\n",
    "        self.conv2 = NNConv(hidden_dim, hidden_dim, edge_net)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim + in_channels_edge, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.edge_mlp[-1].bias.fill_(BIAS_INIT)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x         = self.node_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        h = F.relu(self.fc_in(x))\n",
    "        h = F.relu(self.conv1(h, edge_index, edge_attr))\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "\n",
    "        h_u, h_v = h[edge_index[0]], h[edge_index[1]]\n",
    "        return self.edge_mlp(torch.cat([h_u, h_v, edge_attr], dim=1)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────\n",
    "# 📍 Cell 8 – Training + Evaluation utilities\n",
    "# ────────────────────────────────────────────\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ---- metrics ----------------------------------------------------\n",
    "def calculate_metrics(preds, labels):\n",
    "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    rec  = tp / (tp + fn + 1e-8)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    return prec, rec, f1\n",
    "\n",
    "# ---- evaluation -------------------------------------------------\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, all_logits, all_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss   = criterion(logits, batch.edge_mask.float())\n",
    "            total_loss += loss.item()\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(batch.edge_mask.float())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    probs = torch.sigmoid(all_logits)\n",
    "\n",
    "    # find best threshold on validation\n",
    "    best_f1, best_thr = 0, 0\n",
    "    for thr in np.logspace(-6, -1, 30):\n",
    "        f1 = calculate_metrics((probs > thr).long(), all_labels.long())[2]\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "\n",
    "    prec, rec, f1 = calculate_metrics((probs > best_thr).long(), all_labels.long())\n",
    "    return total_loss / len(loader), {'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "# ---- training + full history -----------------------------------\n",
    "def train_and_evaluate_model(model, model_name,\n",
    "                             train_loader, val_loader,\n",
    "                             device,\n",
    "                             num_epochs=30,\n",
    "                             patience=7,\n",
    "                             lr=5e-4):\n",
    "    \"\"\"\n",
    "    Train `model`, evaluate on `val_loader` each epoch, \n",
    "    and return a history dict with lists for train_loss, val_loss, precision, recall, f1.\n",
    "    \"\"\"\n",
    "    # you can swap in BCEWithLogitsLoss(pos_weight=...) if you like\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss':   [],\n",
    "        'precision':  [],\n",
    "        'recall':     [],\n",
    "        'f1':         []\n",
    "    }\n",
    "\n",
    "    best_f1, wait = 0.0, 0\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # --- training pass ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            # in‑batch sampler for imbalance\n",
    "            pos = batch.edge_mask.nonzero(as_tuple=True)[0]\n",
    "            neg = (batch.edge_mask == 0).nonzero(as_tuple=True)[0]\n",
    "            k   = min(len(neg), 5 * len(pos))\n",
    "            if len(pos)>0 and k>0:\n",
    "                neg_idx = neg[torch.randperm(len(neg), device=neg.device)[:k]]\n",
    "                sel = torch.cat([pos, neg_idx])\n",
    "            else:\n",
    "                sel = torch.arange(len(batch.edge_mask), device=logits.device)\n",
    "\n",
    "            loss = criterion(logits[sel], batch.edge_mask.float()[sel])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # --- validation pass ---\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, device, criterion)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['precision'].append(val_metrics['precision'])\n",
    "        history['recall'].append(val_metrics['recall'])\n",
    "        history['f1'].append(val_metrics['f1'])\n",
    "\n",
    "        print(f\"{model_name} | Ep {epoch:02d}  Train {avg_train_loss:.4f}  \"\n",
    "              f\"ValF1 {val_metrics['f1']:.4f}\")\n",
    "\n",
    "        # early stopping\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1, wait = val_metrics['f1'], 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"⏹️ early stop at {epoch}, best F1={best_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "[GINE] Fold 1/2\n",
      "GINE-F1 | Ep 01  Train 0.3275  ValF1 0.2807\n",
      "GINE-F1 | Ep 02  Train 0.2161  ValF1 0.4466\n",
      "GINE-F1 | Ep 03  Train 0.2022  ValF1 0.3968\n",
      "GINE-F1 | Ep 04  Train 0.1882  ValF1 0.4319\n",
      "GINE-F1 | Ep 05  Train 0.1879  ValF1 0.3199\n",
      "GINE-F1 | Ep 06  Train 0.1772  ValF1 0.3632\n",
      "GINE-F1 | Ep 07  Train 0.1727  ValF1 0.3928\n",
      "GINE-F1 | Ep 08  Train 0.1639  ValF1 0.4337\n",
      "GINE-F1 | Ep 09  Train 0.1615  ValF1 0.3879\n",
      "⏹️ early stop at 9, best F1=0.4466\n",
      "\n",
      "[GINE] Fold 2/2\n",
      "GINE-F2 | Ep 01  Train 0.3523  ValF1 0.2581\n",
      "GINE-F2 | Ep 02  Train 0.2306  ValF1 0.4687\n",
      "GINE-F2 | Ep 03  Train 0.2075  ValF1 0.3874\n",
      "GINE-F2 | Ep 04  Train 0.1929  ValF1 0.3628\n",
      "GINE-F2 | Ep 05  Train 0.1824  ValF1 0.3517\n",
      "GINE-F2 | Ep 06  Train 0.1753  ValF1 0.4235\n",
      "GINE-F2 | Ep 07  Train 0.1778  ValF1 0.3943\n",
      "GINE-F2 | Ep 08  Train 0.1604  ValF1 0.3570\n",
      "GINE-F2 | Ep 09  Train 0.1579  ValF1 0.3818\n",
      "⏹️ early stop at 9, best F1=0.4687\n",
      "\n",
      "[GAT] Fold 1/2\n",
      "GAT-F1 | Ep 01  Train 0.3987  ValF1 0.1258\n",
      "GAT-F1 | Ep 02  Train 0.2777  ValF1 0.1904\n",
      "GAT-F1 | Ep 03  Train 0.2263  ValF1 0.3146\n",
      "GAT-F1 | Ep 04  Train 0.2003  ValF1 0.3637\n",
      "GAT-F1 | Ep 05  Train 0.1941  ValF1 0.3926\n",
      "GAT-F1 | Ep 06  Train 0.1902  ValF1 0.4111\n",
      "GAT-F1 | Ep 07  Train 0.1795  ValF1 0.4333\n",
      "GAT-F1 | Ep 08  Train 0.1828  ValF1 0.4414\n",
      "GAT-F1 | Ep 09  Train 0.1757  ValF1 0.4596\n",
      "GAT-F1 | Ep 10  Train 0.1807  ValF1 0.4100\n",
      "GAT-F1 | Ep 11  Train 0.1764  ValF1 0.4862\n",
      "GAT-F1 | Ep 12  Train 0.1760  ValF1 0.3939\n",
      "GAT-F1 | Ep 13  Train 0.1725  ValF1 0.2922\n",
      "GAT-F1 | Ep 14  Train 0.1716  ValF1 0.3563\n",
      "GAT-F1 | Ep 15  Train 0.1728  ValF1 0.3767\n",
      "GAT-F1 | Ep 16  Train 0.1617  ValF1 0.4082\n",
      "GAT-F1 | Ep 17  Train 0.1767  ValF1 0.4163\n",
      "GAT-F1 | Ep 18  Train 0.1666  ValF1 0.4273\n",
      "⏹️ early stop at 18, best F1=0.4862\n",
      "\n",
      "[GAT] Fold 2/2\n",
      "GAT-F2 | Ep 01  Train 0.3359  ValF1 0.2277\n",
      "GAT-F2 | Ep 02  Train 0.2483  ValF1 0.2955\n",
      "GAT-F2 | Ep 03  Train 0.2170  ValF1 0.3512\n",
      "GAT-F2 | Ep 04  Train 0.2071  ValF1 0.3492\n",
      "GAT-F2 | Ep 05  Train 0.2030  ValF1 0.3819\n",
      "GAT-F2 | Ep 06  Train 0.1884  ValF1 0.3827\n",
      "GAT-F2 | Ep 07  Train 0.1984  ValF1 0.4277\n",
      "GAT-F2 | Ep 08  Train 0.1884  ValF1 0.4208\n",
      "GAT-F2 | Ep 09  Train 0.1945  ValF1 0.3779\n",
      "GAT-F2 | Ep 10  Train 0.1875  ValF1 0.4816\n",
      "GAT-F2 | Ep 11  Train 0.1876  ValF1 0.4262\n",
      "GAT-F2 | Ep 12  Train 0.1943  ValF1 0.3787\n",
      "GAT-F2 | Ep 13  Train 0.1790  ValF1 0.4305\n",
      "GAT-F2 | Ep 14  Train 0.1819  ValF1 0.4344\n",
      "GAT-F2 | Ep 15  Train 0.1773  ValF1 0.4795\n",
      "GAT-F2 | Ep 16  Train 0.1874  ValF1 0.4197\n",
      "GAT-F2 | Ep 17  Train 0.1866  ValF1 0.5100\n",
      "GAT-F2 | Ep 18  Train 0.1718  ValF1 0.4021\n",
      "GAT-F2 | Ep 19  Train 0.1784  ValF1 0.4532\n",
      "GAT-F2 | Ep 20  Train 0.1771  ValF1 0.5040\n",
      "GAT-F2 | Ep 21  Train 0.1735  ValF1 0.5086\n",
      "GAT-F2 | Ep 22  Train 0.1716  ValF1 0.4142\n",
      "GAT-F2 | Ep 23  Train 0.1726  ValF1 0.2626\n",
      "GAT-F2 | Ep 24  Train 0.1686  ValF1 0.4557\n",
      "⏹️ early stop at 24, best F1=0.5100\n",
      "\n",
      "[EdgeAwareGC] Fold 1/2\n",
      "EdgeAwareGC-F1 | Ep 01  Train 0.2420  ValF1 0.3702\n",
      "EdgeAwareGC-F1 | Ep 02  Train 0.1876  ValF1 0.3611\n",
      "EdgeAwareGC-F1 | Ep 03  Train 0.1801  ValF1 0.4017\n",
      "EdgeAwareGC-F1 | Ep 04  Train 0.1850  ValF1 0.4111\n",
      "EdgeAwareGC-F1 | Ep 05  Train 0.1654  ValF1 0.3531\n",
      "EdgeAwareGC-F1 | Ep 06  Train 0.1555  ValF1 0.4348\n",
      "EdgeAwareGC-F1 | Ep 07  Train 0.1475  ValF1 0.4818\n",
      "EdgeAwareGC-F1 | Ep 08  Train 0.1284  ValF1 0.4837\n",
      "EdgeAwareGC-F1 | Ep 09  Train 0.1186  ValF1 0.5232\n",
      "EdgeAwareGC-F1 | Ep 10  Train 0.1212  ValF1 0.4616\n",
      "EdgeAwareGC-F1 | Ep 11  Train 0.1078  ValF1 0.5372\n",
      "EdgeAwareGC-F1 | Ep 12  Train 0.0994  ValF1 0.5109\n",
      "EdgeAwareGC-F1 | Ep 13  Train 0.0904  ValF1 0.5278\n",
      "EdgeAwareGC-F1 | Ep 14  Train 0.0839  ValF1 0.5084\n",
      "EdgeAwareGC-F1 | Ep 15  Train 0.0743  ValF1 0.5877\n",
      "EdgeAwareGC-F1 | Ep 16  Train 0.0717  ValF1 0.4779\n",
      "EdgeAwareGC-F1 | Ep 17  Train 0.0670  ValF1 0.5310\n",
      "EdgeAwareGC-F1 | Ep 18  Train 0.0686  ValF1 0.5952\n",
      "EdgeAwareGC-F1 | Ep 19  Train 0.0679  ValF1 0.5045\n",
      "EdgeAwareGC-F1 | Ep 20  Train 0.0640  ValF1 0.4947\n",
      "EdgeAwareGC-F1 | Ep 21  Train 0.0640  ValF1 0.7122\n",
      "EdgeAwareGC-F1 | Ep 22  Train 0.0518  ValF1 0.7067\n",
      "EdgeAwareGC-F1 | Ep 23  Train 0.0567  ValF1 0.7213\n",
      "EdgeAwareGC-F1 | Ep 24  Train 0.0466  ValF1 0.6215\n",
      "EdgeAwareGC-F1 | Ep 25  Train 0.0469  ValF1 0.5863\n",
      "EdgeAwareGC-F1 | Ep 26  Train 0.0433  ValF1 0.6601\n",
      "EdgeAwareGC-F1 | Ep 27  Train 0.0458  ValF1 0.7650\n",
      "EdgeAwareGC-F1 | Ep 28  Train 0.0451  ValF1 0.8204\n",
      "EdgeAwareGC-F1 | Ep 29  Train 0.0461  ValF1 0.6718\n",
      "EdgeAwareGC-F1 | Ep 30  Train 0.0439  ValF1 0.7883\n",
      "EdgeAwareGC-F1 | Ep 31  Train 0.0406  ValF1 0.5785\n",
      "EdgeAwareGC-F1 | Ep 32  Train 0.0402  ValF1 0.8218\n",
      "EdgeAwareGC-F1 | Ep 33  Train 0.0417  ValF1 0.7441\n",
      "EdgeAwareGC-F1 | Ep 34  Train 0.0365  ValF1 0.8363\n",
      "EdgeAwareGC-F1 | Ep 35  Train 0.0381  ValF1 0.8580\n",
      "EdgeAwareGC-F1 | Ep 36  Train 0.0379  ValF1 0.8031\n",
      "EdgeAwareGC-F1 | Ep 37  Train 0.0341  ValF1 0.7295\n",
      "EdgeAwareGC-F1 | Ep 38  Train 0.0328  ValF1 0.8056\n",
      "EdgeAwareGC-F1 | Ep 39  Train 0.0387  ValF1 0.7186\n",
      "EdgeAwareGC-F1 | Ep 40  Train 0.0366  ValF1 0.7907\n",
      "EdgeAwareGC-F1 | Ep 41  Train 0.0330  ValF1 0.8672\n",
      "EdgeAwareGC-F1 | Ep 42  Train 0.0309  ValF1 0.8464\n",
      "EdgeAwareGC-F1 | Ep 43  Train 0.0328  ValF1 0.8327\n",
      "EdgeAwareGC-F1 | Ep 44  Train 0.0321  ValF1 0.8868\n",
      "EdgeAwareGC-F1 | Ep 45  Train 0.0311  ValF1 0.8778\n",
      "EdgeAwareGC-F1 | Ep 46  Train 0.0335  ValF1 0.7177\n",
      "EdgeAwareGC-F1 | Ep 47  Train 0.0356  ValF1 0.8088\n",
      "EdgeAwareGC-F1 | Ep 48  Train 0.0280  ValF1 0.8778\n",
      "EdgeAwareGC-F1 | Ep 49  Train 0.0322  ValF1 0.8590\n",
      "EdgeAwareGC-F1 | Ep 50  Train 0.0302  ValF1 0.7931\n",
      "EdgeAwareGC-F1 | Ep 51  Train 0.0288  ValF1 0.8610\n",
      "⏹️ early stop at 51, best F1=0.8868\n",
      "\n",
      "[EdgeAwareGC] Fold 2/2\n",
      "EdgeAwareGC-F2 | Ep 01  Train 0.2660  ValF1 0.3945\n",
      "EdgeAwareGC-F2 | Ep 02  Train 0.1976  ValF1 0.4241\n",
      "EdgeAwareGC-F2 | Ep 03  Train 0.1896  ValF1 0.5003\n",
      "EdgeAwareGC-F2 | Ep 04  Train 0.1854  ValF1 0.4443\n",
      "EdgeAwareGC-F2 | Ep 05  Train 0.1780  ValF1 0.3454\n",
      "EdgeAwareGC-F2 | Ep 06  Train 0.1495  ValF1 0.4127\n",
      "EdgeAwareGC-F2 | Ep 07  Train 0.1551  ValF1 0.4523\n",
      "EdgeAwareGC-F2 | Ep 08  Train 0.1390  ValF1 0.4723\n",
      "EdgeAwareGC-F2 | Ep 09  Train 0.1353  ValF1 0.4926\n",
      "EdgeAwareGC-F2 | Ep 10  Train 0.1290  ValF1 0.4478\n",
      "⏹️ early stop at 10, best F1=0.5003\n",
      "\n",
      "📊  Fast 2‑Fold Summary (mean ± std)\n",
      "GINE:  F1 0.3848±0.0031, Prec 0.2409±0.0022, Rec 0.9552±0.0033\n",
      "GAT:  F1 0.4415±0.0142, Prec 0.2899±0.0136, Rec 0.9282±0.0142\n",
      "EdgeAwareGC:  F1 0.6544±0.2066, Prec 0.5293±0.2375, Rec 0.9723±0.0093\n"
     ]
    }
   ],
   "source": [
    "# 📍 Cell 9 – Fast 2‑fold run across models with full metrics saving\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def run_kfold_full(model_name, model_class, dataset, indices,\n",
    "                   K=2, num_epochs=100, patience=7):\n",
    "    \"\"\"\n",
    "    Runs K‑fold CV, returns:\n",
    "      - fold_histories: list of per-fold history dicts\n",
    "      - summary: dict of mean±std for precision, recall, f1 across folds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "    fold_histories = []\n",
    "    all_prec, all_rec, all_f1 = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "        print(f\"\\n[{model_name}] Fold {fold}/{K}\")\n",
    "        # build subsets\n",
    "        tr_ids = [indices[i] for i in train_idx]\n",
    "        vl_ids = [indices[i] for i in val_idx]\n",
    "        tr_ds = torch.utils.data.Subset(dataset, tr_ids)\n",
    "        vl_ds = torch.utils.data.Subset(dataset, vl_ids)\n",
    "\n",
    "        # weighted sampler to handle imbalance\n",
    "        frac_pos = [g.edge_mask.float().mean().item() for g in dataset]\n",
    "        weights = [1.0/(p if p>0 else 1e-4) for p in frac_pos]\n",
    "        sampler = WeightedRandomSampler([weights[i] for i in tr_ids],\n",
    "                                        len(tr_ids), replacement=True)\n",
    "\n",
    "        tr_loader = PyGDataLoader(tr_ds, batch_size=128, sampler=sampler)\n",
    "        vl_loader = PyGDataLoader(vl_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "        # init model\n",
    "        model = model_class().to(device)\n",
    "        # train and get history\n",
    "        history = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            model_name=f\"{model_name}-F{fold}\",\n",
    "            train_loader=tr_loader,\n",
    "            val_loader=vl_loader,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs,\n",
    "            patience=patience\n",
    "        )\n",
    "        # capture final fold metrics (last epoch on validation)\n",
    "        final_prec = history['precision'][-1]\n",
    "        final_rec  = history['recall'][-1]\n",
    "        final_f1   = history['f1'][-1]\n",
    "\n",
    "        all_prec.append(final_prec)\n",
    "        all_rec.append(final_rec)\n",
    "        all_f1.append(final_f1)\n",
    "\n",
    "        # store everything\n",
    "        fold_histories.append({\n",
    "            'fold': fold,\n",
    "            'train_loss': history['train_loss'],\n",
    "            'val_loss':   history['val_loss'],\n",
    "            'precision':  history['precision'],\n",
    "            'recall':     history['recall'],\n",
    "            'f1':         history['f1']\n",
    "        })\n",
    "\n",
    "    # compute mean±std\n",
    "    summary = {\n",
    "        'precision': {'mean': float(np.mean(all_prec)), 'std': float(np.std(all_prec))},\n",
    "        'recall':    {'mean': float(np.mean(all_rec)),  'std': float(np.std(all_rec))},\n",
    "        'f1':        {'mean': float(np.mean(all_f1)),   'std': float(np.std(all_f1))}\n",
    "    }\n",
    "\n",
    "    return fold_histories, summary\n",
    "\n",
    "# ==== run all three models ====================================================\n",
    "model_classes = {\n",
    "    \"GINE\":        GINEBasedClassifier,\n",
    "    \"GAT\":         GATBasedClassifier,\n",
    "    \"EdgeAwareGC\": EdgeAwareGraphConvClassifier\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "for name, cls in model_classes.items():\n",
    "    histories, summary = run_kfold_full(\n",
    "        model_name=name,\n",
    "        model_class=cls,\n",
    "        dataset=full_dataset,\n",
    "        indices=train_idx,\n",
    "        K=2,\n",
    "        num_epochs=100,\n",
    "        patience=7\n",
    "    )\n",
    "    all_results[name] = {\n",
    "        'folds': histories,\n",
    "        'summary': summary\n",
    "    }\n",
    "\n",
    "# save full JSON\n",
    "with open(\"enhanced_full_results_UK_pbs.json\", \"w\") as fp:\n",
    "    json.dump(all_results, fp, indent=2)\n",
    "\n",
    "# print summary\n",
    "print(\"\\n📊  Fast 2‑Fold Summary (mean ± std)\")\n",
    "for m, metrics in all_results.items():\n",
    "    sp = metrics['summary']\n",
    "    print(f\"{m}:  F1 {sp['f1']['mean']:.4f}±{sp['f1']['std']:.4f}, \"\n",
    "          f\"Prec {sp['precision']['mean']:.4f}±{sp['precision']['std']:.4f}, \"\n",
    "          f\"Rec {sp['recall']['mean']:.4f}±{sp['recall']['std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GINE for feature‐importance…\n",
      "GINE-FI | Ep 01  Train 0.7552  ValF1 0.1506\n",
      "GINE-FI | Ep 02  Train 0.4770  ValF1 0.1534\n",
      "GINE-FI | Ep 03  Train 0.3844  ValF1 0.1818\n",
      "GINE-FI | Ep 04  Train 0.3216  ValF1 0.1694\n",
      "GINE-FI | Ep 05  Train 0.2742  ValF1 0.1886\n",
      "GINE-FI | Ep 06  Train 0.2576  ValF1 0.2016\n",
      "GINE-FI | Ep 07  Train 0.2427  ValF1 0.2469\n",
      "GINE-FI | Ep 08  Train 0.2080  ValF1 0.3003\n",
      "GINE-FI | Ep 09  Train 0.2254  ValF1 0.1903\n",
      "GINE-FI | Ep 10  Train 0.1893  ValF1 0.2615\n",
      "GINE-FI | Ep 11  Train 0.2045  ValF1 0.2203\n",
      "GINE-FI | Ep 12  Train 0.1877  ValF1 0.3635\n",
      "GINE-FI | Ep 13  Train 0.1725  ValF1 0.3379\n",
      "GINE-FI | Ep 14  Train 0.1486  ValF1 0.4114\n",
      "GINE-FI | Ep 15  Train 0.1563  ValF1 0.4538\n",
      "GINE-FI | Ep 16  Train 0.1512  ValF1 0.5296\n",
      "GINE-FI | Ep 17  Train 0.1183  ValF1 0.7718\n",
      "GINE-FI | Ep 18  Train 0.1520  ValF1 0.4643\n",
      "GINE-FI | Ep 19  Train 0.1080  ValF1 0.5513\n",
      "GINE-FI | Ep 20  Train 0.1353  ValF1 0.4355\n",
      "GINE-FI | Ep 21  Train 0.1179  ValF1 0.6479\n",
      "GINE-FI | Ep 22  Train 0.1316  ValF1 0.4841\n",
      "GINE-FI | Ep 23  Train 0.1380  ValF1 0.5565\n",
      "GINE-FI | Ep 24  Train 0.1166  ValF1 0.8359\n",
      "GINE-FI | Ep 25  Train 0.1354  ValF1 0.6416\n",
      "GINE-FI | Ep 26  Train 0.1205  ValF1 0.4020\n",
      "GINE-FI | Ep 27  Train 0.1033  ValF1 0.5718\n",
      "GINE-FI | Ep 28  Train 0.1114  ValF1 0.7147\n",
      "GINE-FI | Ep 29  Train 0.0937  ValF1 0.6061\n",
      "GINE-FI | Ep 30  Train 0.0964  ValF1 0.7353\n",
      "GINE-FI | Ep 31  Train 0.1138  ValF1 0.3922\n",
      "GINE-FI | Ep 32  Train 0.0920  ValF1 0.8463\n",
      "GINE-FI | Ep 33  Train 0.1097  ValF1 0.7308\n",
      "GINE-FI | Ep 34  Train 0.0942  ValF1 0.5479\n",
      "GINE-FI | Ep 35  Train 0.0838  ValF1 0.8451\n",
      "GINE-FI | Ep 36  Train 0.1245  ValF1 0.8039\n",
      "GINE-FI | Ep 37  Train 0.1055  ValF1 0.7414\n",
      "GINE-FI | Ep 38  Train 0.0857  ValF1 0.4542\n",
      "GINE-FI | Ep 39  Train 0.0888  ValF1 0.6533\n",
      "GINE-FI | Ep 40  Train 0.1027  ValF1 0.8156\n",
      "GINE-FI | Ep 41  Train 0.1009  ValF1 0.6637\n",
      "GINE-FI | Ep 42  Train 0.1029  ValF1 0.8431\n",
      "⏹️ early stop at 42  best F1=0.8463 @ 32\n",
      "\n",
      "Training GAT for feature‐importance…\n",
      "GAT-FI | Ep 01  Train 0.9883  ValF1 0.0728\n",
      "GAT-FI | Ep 02  Train 0.5175  ValF1 0.1197\n",
      "GAT-FI | Ep 03  Train 0.3948  ValF1 0.1699\n",
      "GAT-FI | Ep 04  Train 0.3464  ValF1 0.1706\n",
      "GAT-FI | Ep 05  Train 0.3351  ValF1 0.1389\n",
      "GAT-FI | Ep 06  Train 0.3159  ValF1 0.1918\n",
      "GAT-FI | Ep 07  Train 0.2593  ValF1 0.1673\n",
      "GAT-FI | Ep 08  Train 0.2656  ValF1 0.2132\n",
      "GAT-FI | Ep 09  Train 0.2389  ValF1 0.1936\n",
      "GAT-FI | Ep 10  Train 0.2254  ValF1 0.2673\n",
      "GAT-FI | Ep 11  Train 0.2527  ValF1 0.2603\n",
      "GAT-FI | Ep 12  Train 0.2150  ValF1 0.4325\n",
      "GAT-FI | Ep 13  Train 0.2108  ValF1 0.3209\n",
      "GAT-FI | Ep 14  Train 0.2013  ValF1 0.2781\n",
      "GAT-FI | Ep 15  Train 0.2148  ValF1 0.3121\n",
      "GAT-FI | Ep 16  Train 0.1951  ValF1 0.2849\n",
      "GAT-FI | Ep 17  Train 0.1863  ValF1 0.3448\n",
      "GAT-FI | Ep 18  Train 0.1846  ValF1 0.3145\n",
      "GAT-FI | Ep 19  Train 0.1764  ValF1 0.2218\n",
      "GAT-FI | Ep 20  Train 0.1815  ValF1 0.3151\n",
      "GAT-FI | Ep 21  Train 0.1811  ValF1 0.3271\n",
      "GAT-FI | Ep 22  Train 0.1604  ValF1 0.3677\n",
      "⏹️ early stop at 22  best F1=0.4325 @ 12\n",
      "\n",
      "Training EdgeAwareGC for feature‐importance…\n",
      "EdgeAwareGC-FI | Ep 01  Train 0.7209  ValF1 0.1296\n",
      "EdgeAwareGC-FI | Ep 02  Train 0.3410  ValF1 0.2167\n",
      "EdgeAwareGC-FI | Ep 03  Train 0.2921  ValF1 0.2337\n",
      "EdgeAwareGC-FI | Ep 04  Train 0.2333  ValF1 0.3083\n",
      "EdgeAwareGC-FI | Ep 05  Train 0.1832  ValF1 0.3482\n",
      "EdgeAwareGC-FI | Ep 06  Train 0.1550  ValF1 0.6195\n",
      "EdgeAwareGC-FI | Ep 07  Train 0.1674  ValF1 0.2823\n",
      "EdgeAwareGC-FI | Ep 08  Train 0.1564  ValF1 0.7177\n",
      "EdgeAwareGC-FI | Ep 09  Train 0.1458  ValF1 0.6967\n",
      "EdgeAwareGC-FI | Ep 10  Train 0.1345  ValF1 0.6397\n",
      "EdgeAwareGC-FI | Ep 11  Train 0.1246  ValF1 0.2751\n",
      "EdgeAwareGC-FI | Ep 12  Train 0.1427  ValF1 0.4998\n",
      "EdgeAwareGC-FI | Ep 13  Train 0.1200  ValF1 0.3801\n",
      "EdgeAwareGC-FI | Ep 14  Train 0.0880  ValF1 0.8062\n",
      "EdgeAwareGC-FI | Ep 15  Train 0.1014  ValF1 0.8424\n",
      "EdgeAwareGC-FI | Ep 16  Train 0.0939  ValF1 0.8119\n",
      "EdgeAwareGC-FI | Ep 17  Train 0.0868  ValF1 0.7656\n",
      "EdgeAwareGC-FI | Ep 18  Train 0.1041  ValF1 0.7332\n",
      "EdgeAwareGC-FI | Ep 19  Train 0.0753  ValF1 0.6882\n",
      "EdgeAwareGC-FI | Ep 20  Train 0.0758  ValF1 0.6370\n",
      "EdgeAwareGC-FI | Ep 21  Train 0.0860  ValF1 0.7463\n",
      "EdgeAwareGC-FI | Ep 22  Train 0.0814  ValF1 0.8166\n",
      "EdgeAwareGC-FI | Ep 23  Train 0.0949  ValF1 0.6887\n",
      "EdgeAwareGC-FI | Ep 24  Train 0.0697  ValF1 0.8561\n",
      "EdgeAwareGC-FI | Ep 25  Train 0.0757  ValF1 0.6632\n",
      "EdgeAwareGC-FI | Ep 26  Train 0.0745  ValF1 0.7947\n",
      "EdgeAwareGC-FI | Ep 27  Train 0.0719  ValF1 0.5949\n",
      "EdgeAwareGC-FI | Ep 28  Train 0.0537  ValF1 0.8054\n",
      "EdgeAwareGC-FI | Ep 29  Train 0.0667  ValF1 0.8517\n",
      "EdgeAwareGC-FI | Ep 30  Train 0.1173  ValF1 0.4996\n",
      "EdgeAwareGC-FI | Ep 31  Train 0.0720  ValF1 0.6742\n",
      "EdgeAwareGC-FI | Ep 32  Train 0.0906  ValF1 0.6878\n",
      "EdgeAwareGC-FI | Ep 33  Train 0.0586  ValF1 0.8582\n",
      "EdgeAwareGC-FI | Ep 34  Train 0.0721  ValF1 0.7996\n",
      "EdgeAwareGC-FI | Ep 35  Train 0.0587  ValF1 0.7609\n",
      "EdgeAwareGC-FI | Ep 36  Train 0.0626  ValF1 0.6906\n",
      "EdgeAwareGC-FI | Ep 37  Train 0.0760  ValF1 0.7561\n",
      "EdgeAwareGC-FI | Ep 38  Train 0.0612  ValF1 0.5123\n",
      "EdgeAwareGC-FI | Ep 39  Train 0.0551  ValF1 0.8386\n",
      "EdgeAwareGC-FI | Ep 40  Train 0.0752  ValF1 0.8311\n",
      "EdgeAwareGC-FI | Ep 41  Train 0.0721  ValF1 0.7565\n",
      "EdgeAwareGC-FI | Ep 42  Train 0.0703  ValF1 0.6624\n",
      "EdgeAwareGC-FI | Ep 43  Train 0.0636  ValF1 0.6803\n",
      "⏹️ early stop at 43  best F1=0.8582 @ 33\n",
      "\n",
      "Analyzing GINE importance…\n",
      "\n",
      "Analyzing GAT importance…\n",
      "\n",
      "Analyzing EdgeAwareGC importance…\n",
      "\n",
      "Node Feature Importance:\n",
      "                   GINE       GAT  EdgeAwareGC\n",
      "Node Feature                                 \n",
      "Orig‐1        0.002292  0.012732     0.001414\n",
      "Orig‐2        0.002093  0.012404     0.001325\n",
      "Orig‐3        0.002349  0.012910     0.001396\n",
      "Node BC       0.001910  0.011294     0.001067\n",
      "Node Deg      0.009829  0.055983     0.006107\n",
      "Volt Dev      0.001769  0.008398     0.000995\n",
      "\n",
      "Edge Feature Importance:\n",
      "                   GINE       GAT  EdgeAwareGC\n",
      "Edge Feature                                 \n",
      "P             0.000314  0.006674     0.000050\n",
      "Q             0.000442  0.004948     0.000009\n",
      "X             0.001326  0.012689     0.000137\n",
      "lr            0.001423  0.019056     0.000247\n",
      "Edge BC       0.000490  0.008376     0.000059\n",
      "Load %        0.001192  0.020566     0.000064\n",
      "Elec BC       0.000529  0.015714     0.000067\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Joint Node & Edge Feature Importance\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Reinit models\n",
    "models = {\n",
    "    \"GINE\":        GINEBasedClassifier(in_channels_node=6, in_channels_edge=7).to(device),\n",
    "    \"GAT\":         GATBasedClassifier(in_channels_node=6, in_channels_edge=7).to(device),\n",
    "    \"EdgeAwareGC\": EdgeAwareGraphConvClassifier(in_channels_node=6, in_channels_edge=7).to(device),\n",
    "}\n",
    "\n",
    "# 2) Train each model on full training set\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} for feature‐importance…\")\n",
    "    train_one_model(\n",
    "        model=model,\n",
    "        name=f\"{name}-FI\",\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        num_epochs=50,\n",
    "        patience=10,\n",
    "        lr=5e-4\n",
    "    )\n",
    "\n",
    "def analyze_importance(model, loader, device,\n",
    "                       in_node_feats=6, in_edge_feats=7, hidden_dim=64):\n",
    "    \"\"\"Return (node_imp, edge_imp) arrays.\"\"\"\n",
    "    model.eval()\n",
    "    # locate the two first‐linear layers:\n",
    "    node_lin = model.fc_in            # [hidden_dim x in_node_feats]\n",
    "    edge_lin = model.edge_mlp[0]      # [hidden_dim x (2*hidden_dim + in_edge_feats)]\n",
    "\n",
    "    # precompute slice indices for edge‐features\n",
    "    start = 2 * hidden_dim\n",
    "    end   = start + in_edge_feats\n",
    "\n",
    "    node_grads = torch.zeros(in_node_feats, device=device)\n",
    "    edge_grads = torch.zeros(in_edge_feats, device=device)\n",
    "\n",
    "    for batch in loader:\n",
    "        model.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = criterion(logits, batch.edge_mask.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # Node feature importance: sum abs weight‐grads over hidden_dim\n",
    "        wg_node = node_lin.weight.grad.abs().sum(dim=0)  # size in_node_feats\n",
    "        node_grads += wg_node\n",
    "\n",
    "        # Edge feature importance: sum abs weight‐grads only on those last columns\n",
    "        wg_edge = edge_lin.weight.grad.abs().sum(dim=0)[start:end]  # size in_edge_feats\n",
    "        edge_grads += wg_edge\n",
    "\n",
    "    # normalize by number of graphs\n",
    "    N = len(loader.dataset)\n",
    "    return (node_grads.cpu().numpy()/N,\n",
    "            edge_grads.cpu().numpy()/N)\n",
    "\n",
    "# 3) Compute for each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nAnalyzing {name} importance…\")\n",
    "    n_imp, e_imp = analyze_importance(model, train_loader, device)\n",
    "    results[name] = (n_imp, e_imp)\n",
    "\n",
    "# 4) Build DataFrame\n",
    "node_names = [\"Orig‐1\",\"Orig‐2\",\"Orig‐3\",\"Node BC\",\"Node Deg\",\"Volt Dev\"]\n",
    "edge_names = [\"P\",\"Q\",\"X\",\"lr\",\"Edge BC\",\"Load %\",\"Elec BC\"]\n",
    "\n",
    "df_node = pd.DataFrame({m: r[0] for m, r in results.items()}, index=node_names)\n",
    "df_edge = pd.DataFrame({m: r[1] for m, r in results.items()}, index=edge_names)\n",
    "\n",
    "df_node.index.name = \"Node Feature\"\n",
    "df_edge.index.name = \"Edge Feature\"\n",
    "\n",
    "# 5) Save & print\n",
    "df_node.to_csv(\"node_importance.csv\")\n",
    "df_edge.to_csv(\"edge_importance.csv\")\n",
    "\n",
    "print(\"\\nNode Feature Importance:\\n\", df_node)\n",
    "print(\"\\nEdge Feature Importance:\\n\", df_edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subgraph index to visualize (you can change it)\n",
    "i_viz = 42\n",
    "\n",
    "# Get the subgraph from your enhanced full_dataset\n",
    "sample_graph = full_dataset[i_viz]\n",
    "\n",
    "# Print graph stats for context\n",
    "print(\"Subgraph info:\")\n",
    "print(f\"Nodes: {sample_graph.num_nodes}, Edges: {sample_graph.num_edges}\")\n",
    "print(f\"Graph-level label (y): {sample_graph.y.item()}\")\n",
    "print(f\"Unique edge_mask values: {sample_graph.edge_mask.unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Convert to NetworkX for plotting (undirected)\n",
    "G = to_networkx(sample_graph, to_undirected=True)\n",
    "\n",
    "# Map node features for coloring or sizing (e.g., betweenness or voltage deviation)\n",
    "node_color = sample_graph.x[:, -1].numpy()  # Voltage deviation (last column)\n",
    "\n",
    "# Map edge status: 1 = failed during cascade (cascading edge)\n",
    "edge_color = [\n",
    "    'red' if sample_graph.edge_mask[i].item() == 1 else 'gray'\n",
    "    for i in range(sample_graph.edge_index.shape[1])\n",
    "]\n",
    "\n",
    "# Create layout for nice visuals\n",
    "pos = nx.spring_layout(G, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Draw edges and labels\n",
    "nx.draw_networkx_edges(G, pos, edge_color='black')\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "# Draw nodes with color map\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G, pos,\n",
    "    node_color=node_color,\n",
    "    cmap='coolwarm',\n",
    "    node_size=300\n",
    ")\n",
    "\n",
    "# Add colorbar using the actual mappable from draw_networkx_nodes\n",
    "plt.colorbar(nodes, label='Voltage Deviation')\n",
    "\n",
    "plt.title(\"Power Grid Topology (Node color: Voltage Deviation)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# Set clean seaborn styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Recreate failure steps\n",
    "random.seed(42)\n",
    "failed_edges_idx = [i for i, val in enumerate(sample_graph.edge_mask) if val == 1]\n",
    "random.shuffle(failed_edges_idx)\n",
    "\n",
    "step1 = failed_edges_idx[:len(failed_edges_idx)//3]\n",
    "step2 = failed_edges_idx[len(failed_edges_idx)//3:2*len(failed_edges_idx)//3]\n",
    "step3 = failed_edges_idx[2*len(failed_edges_idx)//3:]\n",
    "\n",
    "cascade_steps = {\n",
    "    \"Initial Outage\": step1,\n",
    "    \"Step 1 Propagation\": step2,\n",
    "    \"Step 2 Propagation\": step3\n",
    "}\n",
    "final_step = failed_edges_idx  # All failures\n",
    "\n",
    "# Voltage deviation for node coloring\n",
    "node_vals = sample_graph.x[:, -1].numpy()\n",
    "\n",
    "# Plot setup\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "titles = list(cascade_steps.keys()) + [\"Final Cascade State\"]\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "edges = list(G.edges())\n",
    "\n",
    "# Loop through each subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # Get edges to highlight for this step\n",
    "    if i < 3:\n",
    "        highlight = cascade_steps[titles[i]]\n",
    "    else:\n",
    "        highlight = final_step\n",
    "\n",
    "    # Edge coloring\n",
    "    edge_colors = ['crimson' if idx in highlight else '#cccccc' for idx in range(len(edges))]\n",
    "\n",
    "    # Draw components\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_color=node_vals, cmap='coolwarm', node_size=400, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=2.5, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=9, ax=ax)\n",
    "\n",
    "    ax.set_title(titles[i], fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add legend only in final plot\n",
    "    if i == 3:\n",
    "        legend_elements = [\n",
    "            mpatches.Patch(color='crimson', label='Failed Edge'),\n",
    "            mpatches.Patch(color='#cccccc', label='Active Edge'),\n",
    "            mpatches.Patch(color='blue', label='Low Voltage Deviation'),\n",
    "            mpatches.Patch(color='red', label='High Voltage Deviation')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower left', fontsize=10)\n",
    "\n",
    "# Add shared colorbar for node voltage deviation\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])  # [left, bottom, width, height]\n",
    "sm = ScalarMappable(cmap='coolwarm')\n",
    "sm.set_array(node_vals)\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.set_label(\"Voltage Deviation\", fontsize=12)\n",
    "\n",
    "# Title & layout\n",
    "plt.suptitle(\"Simulated Cascading Failure Progression (Edge Failures Over Steps)\", fontsize=18, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
